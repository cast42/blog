{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About me","text":"<p>Welcome to my blog! I'm a Machine Learning Engineer passionate about building intelligent systems and optimizing workflows through automation. I spend most of my days coding in Python, experimenting with new ML models, and finding efficient ways to deploy AI solutions.</p>"},{"location":"about/#what-i-write-about","title":"What I Write About","text":"<p>On this blog, I share insights on:</p> <ul> <li>Machine Learning &amp; AI \u2013 model training, deployment strategies, and optimization techniques.</li> <li>MLOps &amp; Cloud Computing \u2013 managing models in production, leveraging tools like Azure Cosmos DB and serverless architectures.</li> <li>Software Development \u2013 Python tips, UI development (e.g., using Reflex.dev), and database integrations.</li> <li>Cycling &amp; Sports Analytics \u2013 analyzing lactate production data, power metrics, and Strava integrations.</li> <li>Personal Projects &amp; Experiments \u2013 such as setting up MkDocs with GitHub Actions for this blog!</li> </ul>"},{"location":"about/#tech-i-use","title":"Tech I Use","text":"<p>Some of my favorite tools and frameworks include:</p> <ul> <li>Hugging Face models for NLP and embeddings (e.g., <code>gretelai/gretel-gliner-bi-large-v1.0</code>, <code>cde-small-v2</code>)</li> <li>Gradio for interactive ML interfaces</li> <li>Mesop, Flask, and containerized deployments</li> <li>Reflex.dev</li> </ul>"},{"location":"about/#lets-connect","title":"Let's Connect","text":"<p>Feel free to check out my work on GitHub or reach out for a chat about ML, coding, or cycling.</p> <p>Happy coding! \ud83d\ude80</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"2025/03/16/hello-world/","title":"Hello world","text":"<p>I see a post on bluesky from Vincent Warmerdam: https://bsky.app/profile/koaning.bsky.social/post/3lkgsicxmhj2z</p> <p>He vibe coded a blogpost editor. That sparked me to revamp my old Fastpages blog: https://cast42.github.io/fastpages-blog/</p> <p>This time, I used mkdocs to get my Markdown notes on github pages. I reused the setup from Antonio Cuni</p>"},{"location":"2020/01/28/simple-notebook-with-interactive-altair-graph/","title":"Simple Notebook with interactive Altair graph","text":"<p>Trying basic notebook with interactive graph of the iris dataset</p> <p>Test fast template notebook posts</p> <pre><code>import altair as alt\n</code></pre> <p>Make an Altair graph</p> <pre><code>from vega_datasets import data\niris = data.iris()\n\nalt.Chart(iris).mark_point().encode(\n    x='petalLength',\n    y='petalWidth',\n    color='species',\n    tooltip='species'\n).interactive()\n</code></pre> <p>Enjoy :)</p>"},{"location":"2020/01/29/bullet-chart-in-python-altair/","title":"Bullet chart in python Altair","text":"<p>How to make bullet charts with Altair</p> <p>In the article \"Bullet Charts - What Is It And How To Use It\" I learned about Bullet charts. It's a specific kind of barchart that must convey the state of a measure or KPI. The goal is to see in a glance if the target is met.  Here is an example bullet chart from the article:</p> <pre><code># This causes issues to: \n# from IPython.display import Image\n# Image('https://jscharting.com/blog/bullet-charts/images/bullet_components.png')\n</code></pre> <p></p> <pre><code># &lt;img src=\"https://jscharting.com/blog/bullet-charts/images/bullet_components.png\" alt=\"Bullet chart\" style=\"width: 200px;\"/&gt;\n</code></pre> <p>Below is some Python code that generates bullets graphs using the Altair library.</p> <pre><code>import altair as alt\nimport pandas as pd\n\ndf = pd.DataFrame.from_records([\n    {\"title\":\"Revenue\",\"subtitle\":\"US$, in thousands\",\"ranges\":[150,225,300],\"measures\":[220,270],\"markers\":[250]},\n    {\"title\":\"Profit\",\"subtitle\":\"%\",\"ranges\":[20,25,30],\"measures\":[21,23],\"markers\":[26]},\n    {\"title\":\"Order Size\",\"subtitle\":\"US$, average\",\"ranges\":[350,500,600],\"measures\":[100,320],\"markers\":[550]},\n    {\"title\":\"New Customers\",\"subtitle\":\"count\",\"ranges\":[1400,2000,2500],\"measures\":[1000,1650],\"markers\":[2100]},\n    {\"title\":\"Satisfaction\",\"subtitle\":\"out of 5\",\"ranges\":[3.5,4.25,5],\"measures\":[3.2,4.7],\"markers\":[4.4]}\n])\n\nalt.layer(\n    alt.Chart().mark_bar(color='#eee').encode(alt.X(\"ranges[2]:Q\", scale=alt.Scale(nice=False), title=None)),\n    alt.Chart().mark_bar(color='#ddd').encode(x=\"ranges[1]:Q\"),\n    alt.Chart().mark_bar(color='#bbb').encode(x=\"ranges[0]:Q\"),\n    alt.Chart().mark_bar(color='steelblue', size=10).encode(x='measures[0]:Q'),\n    alt.Chart().mark_tick(color='black', size=12).encode(x='markers[0]:Q'),\n    data=df\n).facet(\n    row=alt.Row(\"title:O\", title='')\n).resolve_scale(\n    x='independent'\n)\n</code></pre>"},{"location":"2020/02/15/evolution-of-burglary-in-leuven-is-the-trend-downwards-/","title":"Evolution of burglary in Leuven. Is the trend downwards ?","text":"<p>Evolution of burglary in Leuven. Is the trend downwards ?</p> <p>The local police shared a graph with the number of break-ins in Leuven per year.  The article shows a graph with a downwards trendline. Can we conclude that the number of breakins is showing a downward trend based on those numbers? Let's construct a dataframe with the data from the graph.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport altair as alt\n\ndf = pd.DataFrame({'year_int':[y for y in range(2006, 2020)], \n                  'breakins':[1133,834,953,891,1006,1218,992,1079,1266,1112,713,669,730,644]})\ndf['year'] = pd.to_datetime(df['year_int'], format='%Y')\n</code></pre> <pre><code>points = alt.Chart(df).mark_line(point=True).encode(\n    x='year', y='breakins', tooltip='breakins'\n)\npoints + points.transform_regression('year', 'breakins').mark_line(\n    color='green'\n).properties(\n    title='Regression trend on the number breakins per year in Leuven'\n)\n</code></pre> <p>The article claims that the number of breakins stabilizes the last years. Let's perform a local regression to check that.</p> <pre><code># https://opendatascience.com/local-regression-in-python\n# Loess: https://gist.github.com/AllenDowney/818f6153ef316aee80467c51faee80f8\npoints + points.transform_loess('year', 'breakins').mark_line(\n    color='green'\n).properties(\n    title='Local regression trend on the number breakins per year in Leuven'\n)\n</code></pre> <p>But what about the trend line? Are we sure the trend is negative ? Bring in the code based on the blogpost The hacker's guide to uncertainty estimates to estimate the uncertainty.:</p> <pre><code># Code from: https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html\nimport scipy.optimize\nimport random\n\ndef model(xs, k, m):\n    return k * xs + m\n\ndef neg_log_likelihood(tup, xs, ys):\n    # Since sigma &gt; 0, we use use log(sigma) as the parameter instead.\n    # That way we have an unconstrained problem.\n    k, m, log_sigma = tup\n    sigma = np.exp(log_sigma)\n    delta = model(xs, k, m) - ys\n    return len(xs)/2*np.log(2*np.pi*sigma**2) + \\\n        np.dot(delta, delta) / (2*sigma**2)\n\ndef confidence_bands(xs, ys, nr_bootstrap):\n    curves = []\n    xys = list(zip(xs, ys))\n    for i in range(nr_bootstrap):\n        # sample with replacement\n        bootstrap = [random.choice(xys) for _ in xys]\n        xs_bootstrap = np.array([x for x, y in bootstrap])\n        ys_bootstrap = np.array([y for x, y in bootstrap])\n        k_hat, m_hat, log_sigma_hat = scipy.optimize.minimize(\n          neg_log_likelihood, (0, 0, 0), args=(xs_bootstrap, ys_bootstrap)\n        ).x\n        curves.append(\n          model(xs, k_hat, m_hat) +\n          # Note what's going on here: we're _adding_ the random term\n          # to the predictions!\n          np.exp(log_sigma_hat) * np.random.normal(size=xs.shape)\n        )\n    lo, hi = np.percentile(curves, (2.5, 97.5), axis=0)\n    return lo, hi\n</code></pre> <pre><code># Make a plot with a confidence band\ndf['lo'], df['hi'] = confidence_bands(df.index, df['breakins'], 100)\n\nci = alt.Chart(df).mark_area().encode(\n    x=alt.X('year:T', title=''),\n    y=alt.Y('lo:Q'),\n    y2=alt.Y2('hi:Q', title=''),\n    color=alt.value('lightblue'),\n    opacity=alt.value(0.6)\n)\n\nchart = alt.Chart(df).mark_line(point=True).encode(\n    x='year', y='breakins', tooltip='breakins'\n)\nci + chart  + chart.transform_regression('year', 'breakins').mark_line(\n    color='red'\n).properties(\n    title='95% Confidence band of the number of breakins per year in Leuven'\n)\n</code></pre> <p>On the above chart, we see that a possitive trend might be possible as well.</p>"},{"location":"2020/02/15/evolution-of-burglary-in-leuven-is-the-trend-downwards-/#linear-regression","title":"Linear regression","text":"<p>Let's perform a linear regression with statsmodel to calculate the confidence interval on the slope of the regression line.</p> <pre><code>import statsmodels.formula.api as smf\n</code></pre> <pre><code>results = smf.ols('breakins ~ index', data=df.reset_index()).fit()\n</code></pre> <pre><code>results.params\n</code></pre> <pre><code>Intercept    1096.314286\nindex         -23.169231\ndtype: float64\n</code></pre> <p>The most likely slope of the trend line is 23.17 breakins per year. But how sure are we that the trend is heading down ?</p> <pre><code>results.summary()\n</code></pre> <pre><code>C:\\Users\\lnh6dt5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1535: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=14\n  \"anyway, n=%i\" % int(n))\n</code></pre> OLS Regression Results Dep. Variable: breakins   R-squared:             0.223 Model: OLS   Adj. R-squared:        0.159 Method: Least Squares   F-statistic:           3.451 Date: Sun, 19 Apr 2020   Prob (F-statistic): 0.0879 Time: 10:26:45   Log-Likelihood:      -92.105 No. Observations:     14   AIC:                   188.2 Df Residuals:     12   BIC:                   189.5 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] Intercept  1096.3143    95.396    11.492  0.000   888.465  1304.164 index   -23.1692    12.472    -1.858  0.088   -50.344     4.006 Omnibus:  1.503   Durbin-Watson:         1.035 Prob(Omnibus):  0.472   Jarque-Bera (JB):      1.196 Skew:  0.577   Prob(JB):              0.550 Kurtosis:  2.153   Cond. No.               14.7 <p>Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</p> <p>The analysis reveals that the slope of the best fitting regression line is 23 breakins less per year. However, the confidence interval of the trend is between -50.344 and 4.006. Also the p)value of the regression coefficient is 0.088. Meaning we have eight percent chance that the negative trend is by accident. Hence, based on the current data we are not 95% percent sure the trend is downwards. Hence we can not conclude, based on this data, that there is a negative trend. This corresponds with the width of the 95% certainty band drawn that allows for an upward trend line:</p> <pre><code># Here are the confidence intervals of the regression\nresults.conf_int()\n</code></pre> <p> 0 1 Intercept 888.464586 1304.163986 index -50.344351 4.005889 </p> <pre><code>y_low  = results.params['Intercept'] # ?ost likely value of the intercept\ny_high = results.params['Intercept'] + results.conf_int()[1]['index'] * df.shape[0] # Value of upward trend for the last year\ndf_upward_trend = pd.DataFrame({'year':[df['year'].min(), df['year'].max()], \n                                'breakins':[y_low, y_high]})\npossible_upwards_trend = alt.Chart(df_upward_trend).mark_line(\n    color='green',\n    strokeDash=[10,10]\n).encode(\n    x='year:T',\n    y=alt.Y('breakins:Q',\n    title='Number of breakins per year')\n)\n\npoints = alt.Chart(df).mark_line(point=True).encode(x='year', y='breakins', tooltip='breakins')\n(ci + points  + points.transform_regression('year', 'breakins').mark_line(color='red') \n              + possible_upwards_trend).properties(\n    title='Trend analysis on the number of breakins per year in Leuven, Belgium'\n)\n</code></pre> <p>In the above graph, we see that a slight positive trend (green dashed line) is in the 95% confidence band on the regression coefficient. We are not sure that the trend on the number of breakins is downwards.</p> <pre><code>\n</code></pre>"},{"location":"2020/04/18/first-test-post/","title":"First test post","text":"<p>Testing a simple notebook for publishing with fastpages</p> <pre><code>import pandas as pd\nimport altair as alt\n</code></pre> <pre><code># Check if this get published\n</code></pre>"},{"location":"2020/04/19/daily-covid-19-deaths-compared-to-average-deaths-the-last-10-years/","title":"Daily covid-19 Deaths compared to average deaths the last 10 years","text":"<p>\"In this blogpost we try to get an idea of how many extra deaths we have in Belgium due to covid-19 compared to the average we had the last 10 years.\"</p> <pre><code># Import pandas for data wrangling and Altair for plotting\nimport pandas as pd\nimport altair as alt\n</code></pre> <p>The number of deadths per day from 2008 until 2018 can obtained from Statbel, the Belgium federal bureau of statistics:</p> <pre><code>df = pd.read_excel('https://statbel.fgov.be/sites/default/files/files/opendata/bevolking/TF_DEATHS.xlsx') # , skiprows=5, sheet_name=sheetnames\n</code></pre> <pre><code># Get a quick look to the data\ndf.head()\n</code></pre> DT_DATE MS_NUM_DEATHS 0 2008-01-01 342 1 2008-01-02 348 2 2008-01-03 340 3 2008-01-04 349 4 2008-01-05 348 <pre><code>df['Jaar'] = df['DT_DATE'].dt.year\ndf['Dag'] = df['DT_DATE'].dt.dayofyear\n</code></pre> <pre><code>df_plot = df.groupby('Dag')['MS_NUM_DEATHS'].mean().to_frame().reset_index()\n</code></pre> <pre><code># Let's make a quick plot\nalt.Chart(df_plot).mark_line().encode(x='Dag', y='MS_NUM_DEATHS').properties(width=600)\n</code></pre> <p>The John Hopkings University CSSE keeps track of the number of covid-19 deadths per day and country in a github repository: https://github.com/CSSEGISandData/COVID-19. We can easily obtain this data by reading it from github and filter out the cases for Belgium.</p> <pre><code>deaths_url =  'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\ndeaths = pd.read_csv(deaths_url, sep=',')\n</code></pre> <p>Filter out Belgium</p> <pre><code>deaths_be = deaths[deaths['Country/Region'] == 'Belgium']\n</code></pre> <p>Inspect how the data is stored</p> <pre><code>deaths_be\n</code></pre> Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/9/20 4/10/20 4/11/20 4/12/20 4/13/20 4/14/20 4/15/20 4/16/20 4/17/20 4/18/20 23 NaN Belgium 50.8333 4.0 0 0 0 0 0 0 ... 2523 3019 3346 3600 3903 4157 4440 4857 5163 5453 <p>1 rows \u00d7 92 columns</p> <p>Create dateframe for plotting</p> <pre><code>df_deaths = pd.DataFrame(data={'Datum':pd.to_datetime(deaths_be.columns[4:]), 'Overlijdens':deaths_be.iloc[0].values[4:]})\n</code></pre> <p>Check for Nan's</p> <pre><code>df_deaths['Overlijdens'].isna().sum()\n</code></pre> <pre><code>0\n</code></pre> <p>We need to do some type convertions. We cast 'Overlijdens' to integer. Next, we add the number of the day.</p> <pre><code>df_deaths['Overlijdens'] = df_deaths['Overlijdens'].astype(int)\ndf_deaths['Dag'] = df_deaths['Datum'].dt.dayofyear\n</code></pre> <p>Plot the data:</p> <pre><code>dead_2008_2018 = alt.Chart(df_plot).mark_line().encode(x='Dag', y='MS_NUM_DEATHS')\ndead_2008_2018\n</code></pre> <p>Calculate the day-by-day change</p> <pre><code>df_deaths['Nieuwe covid-19 Sterfgevallen'] = df_deaths['Overlijdens'].diff()\n</code></pre> <pre><code># Check types\ndf_deaths.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 88 entries, 0 to 87\nData columns (total 4 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   Datum                          88 non-null     datetime64[ns]\n 1   Overlijdens                    88 non-null     int32         \n 2   Dag                            88 non-null     int64         \n 3   Nieuwe covid-19 Sterfgevallen  87 non-null     float64       \ndtypes: datetime64[ns](1), float64(1), int32(1), int64(1)\nmemory usage: 2.5 KB\n</code></pre> <p>Plot covid-19 deaths in Belgium according to JHU CSSE. The plot shows a tooltip if you hover over the points.</p> <pre><code>dead_covid= alt.Chart(df_deaths).mark_line(point=True).encode(\n    x=alt.X('Dag',scale=alt.Scale(domain=(1, 110), clamp=True)),\n    y='Nieuwe covid-19 Sterfgevallen', \n    color=alt.ColorValue('red'), \n    tooltip=['Dag', 'Nieuwe covid-19 Sterfgevallen'])\ndead_covid\n</code></pre> <p>Now we add average deaths per day in the last 10 year to the plot.</p> <pre><code>dead_2008_2018 + dead_covid\n</code></pre> <p>Take quick look to the datatable:</p> <pre><code>df.head()\n</code></pre> DT_DATE MS_NUM_DEATHS Jaar Dag 0 2008-01-01 342 2008 1 1 2008-01-02 348 2008 2 2 2008-01-03 340 2008 3 3 2008-01-04 349 2008 4 4 2008-01-05 348 2008 5 <p>The column 'DT_DATE' is a string. We convert it to a datatime so we can add it to the tooltip.</p> <pre><code>df['Datum'] = pd.to_datetime(df['DT_DATE'])\n</code></pre> <p>Now we are prepared to make the final graph. We use the Altair mark_errorband(extend='ci') to bootstrap 95% confidence band around the average number of deaths per day.</p> <pre><code>line = alt.Chart(df).mark_line().encode(\n    x=alt.X('Dag', scale=alt.Scale(\n            domain=(1, 120),\n            clamp=True\n        )),\n    y='mean(MS_NUM_DEATHS)'\n)\n\n# Bootstrapped 95% confidence interval\nband = alt.Chart(df).mark_errorband(extent='ci').encode(\n    x=alt.X('Dag', scale=alt.Scale(domain=(1, 120), clamp=True)),\n    y=alt.Y('MS_NUM_DEATHS', title='Overlijdens per dag'),\n)\n\ndead_covid= alt.Chart(df_deaths).mark_line(point=True).encode(\n    x=alt.X('Dag',scale=alt.Scale(domain=(1, 120), clamp=True)),\n    y='Nieuwe covid-19 Sterfgevallen',\n    color=alt.ColorValue('red'),\n    tooltip=['Dag', 'Nieuwe covid-19 Sterfgevallen', 'Datum']\n)\n\n(band + line + dead_covid).properties(width=1024, title='Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie')\n</code></pre>"},{"location":"2020/04/19/daily-covid-19-deaths-compared-to-average-deaths-the-last-10-years/#source-date-from-sciensano","title":"Source date from sciensano","text":"<p>In this section, we compare the graph obtained with data obtained from sciensano.</p> <pre><code>df_sc = pd.read_csv('https://epistat.sciensano.be/Data/COVID19BE_MORT.csv')\n</code></pre> <pre><code>df_sc.head()\n</code></pre> DATE REGION AGEGROUP SEX DEATHS 0 2020-03-10 Brussels 85+ F 1 1 2020-03-11 Flanders 85+ F 1 2 2020-03-11 Brussels 75-84 M 1 3 2020-03-11 Brussels 85+ F 1 4 2020-03-12 Brussels 75-84 M 1 <pre><code>df_dead_day = df_sc.groupby('DATE')['DEATHS'].sum().reset_index()\ndf_dead_day['Datum'] = pd.to_datetime(df_dead_day['DATE'])\ndf_dead_day['Dag'] = df_dead_day['Datum'].dt.dayofyear\n</code></pre> <pre><code>line = alt.Chart(df).mark_line().encode(\n    x=alt.X('Dag', title='Dag van het jaar', scale=alt.Scale(\n            domain=(1, 120),\n            clamp=True\n        )),\n    y='mean(MS_NUM_DEATHS)'\n)\n\n# Bootstrapped 95% confidence interval\nband = alt.Chart(df).mark_errorband(extent='ci').encode(\n    x=alt.X('Dag', scale=alt.Scale(domain=(1, 120), clamp=True)),\n    y=alt.Y('MS_NUM_DEATHS', title='Overlijdens per dag'),\n)\n\ndead_covid= alt.Chart(df_dead_day).mark_line(point=True).encode(\n    x=alt.X('Dag',scale=alt.Scale(domain=(1, 120), clamp=True)),\n    y='DEATHS',\n    color=alt.ColorValue('red'),\n    tooltip=['Dag', 'DEATHS', 'Datum']\n)\n\n(band + line + dead_covid).properties(width=750, title='Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie')\n</code></pre> <p>Obviously, data form 16-17-18 April 2020 is not final yet. Also, the amounts are smaller then those from JHU.</p>"},{"location":"2020/04/19/daily-covid-19-deaths-compared-to-average-deaths-the-last-10-years/#obtain-more-detail-for-another-blogpost","title":"Obtain more detail (for another blogpost...)","text":"<pre><code>df_tot_sc = pd.read_excel('https://epistat.sciensano.be/Data/COVID19BE.xlsx')\n</code></pre> <pre><code>df_tot_sc\n</code></pre> DATE PROVINCE REGION AGEGROUP SEX CASES 0 2020-03-01 Brussels Brussels 10-19 M 1 1 2020-03-01 Brussels Brussels 10-19 F 1 2 2020-03-01 Brussels Brussels 20-29 M 1 3 2020-03-01 Brussels Brussels 30-39 F 1 4 2020-03-01 Brussels Brussels 40-49 F 1 ... ... ... ... ... ... ... 6875 NaN OostVlaanderen Flanders NaN F 4 6876 NaN VlaamsBrabant Flanders 40-49 M 3 6877 NaN VlaamsBrabant Flanders 40-49 F 2 6878 NaN VlaamsBrabant Flanders 50-59 M 1 6879 NaN WestVlaanderen Flanders 50-59 M 3 <p>6880 rows \u00d7 6 columns</p> <p>We know that there are a lot of reional differences:</p> <pre><code>df_plot = df_tot_sc.groupby(['DATE', 'PROVINCE'])['CASES'].sum().reset_index()\n</code></pre> <pre><code>df_plot\n</code></pre> DATE PROVINCE CASES 0 2020-03-01 Brussels 6 1 2020-03-01 Limburg 1 2 2020-03-01 Li\u00e8ge 2 3 2020-03-01 OostVlaanderen 1 4 2020-03-01 VlaamsBrabant 6 ... ... ... ... 505 2020-04-17 OostVlaanderen 44 506 2020-04-17 VlaamsBrabant 42 507 2020-04-17 WestVlaanderen 30 508 2020-04-18 Brussels 1 509 2020-04-18 Hainaut 1 <p>510 rows \u00d7 3 columns</p> <pre><code>df_plot['DATE'] = pd.to_datetime(df_plot['DATE'])\n</code></pre> <pre><code>base = alt.Chart(df_plot, title='Number of cases in Belgium per day and province').mark_line(point=True).encode(\n    x=alt.X('DATE:T', title='Datum'),\n    y=alt.Y('CASES', title='Cases per day'),\n    color='PROVINCE',\n    tooltip=['DATE', 'CASES', 'PROVINCE']\n).properties(width=600)\nbase\n</code></pre> <p>From the above graph we see a much lower number of cases in Luxembourg, Namur, Waals Brabant.</p> <pre><code>!pwd\n</code></pre> <pre><code>'pwd' is not recognized as an internal or external command,\noperable program or batch file.\n</code></pre> <pre><code>!dir\n</code></pre> <pre><code> Volume in drive C is Windows\n Volume Serial Number is 7808-E933\n\n Directory of C:\\Users\\lnh6dt5\\AppData\\Local\\Temp\\Mxt121\\tmp\\home_lnh6dt5\\blog\\_notebooks\n\n19/04/2020  14:14    &lt;DIR&gt;          .\n19/04/2020  14:14    &lt;DIR&gt;          ..\n19/04/2020  10:37    &lt;DIR&gt;          .ipynb_checkpoints\n19/04/2020  10:17            23.473 2020-01-28-Altair.ipynb\n19/04/2020  10:34             9.228 2020-01-29-bullet-chart-altair.ipynb\n19/04/2020  10:26            41.041 2020-02-15-breakins.ipynb\n19/04/2020  09:43            30.573 2020-02-20-test.ipynb\n19/04/2020  09:49             1.047 2020-04-18-first-test.ipynb\n19/04/2020  14:14         1.237.674 2020-04-19-deads-last-ten-year-vs-covid.ipynb\n19/04/2020  09:43    &lt;DIR&gt;          my_icons\n19/04/2020  09:43               771 README.md\n               7 File(s)      1.343.807 bytes\n               4 Dir(s)  89.905.336.320 bytes free\n</code></pre> <pre><code>\n</code></pre>"},{"location":"2020/04/22/regional-covid-19-mortality-in-belgium-per-gender-and-age/","title":"Regional covid-19 mortality in Belgium per gender and age","text":"<p>Combines the mortality number of the last 10 year with those of covid-19 this year.</p> <pre><code># Import pandas for data wrangling and Altair for plotting\nimport pandas as pd\nimport altair as alt\n</code></pre> <pre><code>df_tot_sc = pd.read_excel('https://epistat.sciensano.be/Data/COVID19BE.xlsx')\n</code></pre> <pre><code>df_inhab = pd.read_excel('https://statbel.fgov.be/sites/default/files/files/opendata/bevolking%20naar%20woonplaats%2C%20nationaliteit%20burgelijke%20staat%20%2C%20leeftijd%20en%20geslacht/TF_SOC_POP_STRUCT_2019.xlsx')\n</code></pre> <pre><code>df_inhab\n</code></pre> CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION 0 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 69 11 1 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 80 3 2 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande M BEL Belgen Belges 4 Gescheiden Divorc\u00e9 30 2 3 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 48 26 4 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 76 2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 463376 93090 Viroinval Viroinval 93000 Arrondissement Philippeville Arrondissement de Philippeville 90000.0 Provincie Namen Province de Namur 3000 Waals Gewest R\u00e9gion wallonne F BEL Belgen Belges 3 Weduwstaat Veuf 73 10 463377 93090 Viroinval Viroinval 93000 Arrondissement Philippeville Arrondissement de Philippeville 90000.0 Provincie Namen Province de Namur 3000 Waals Gewest R\u00e9gion wallonne M BEL Belgen Belges 3 Weduwstaat Veuf 64 1 463378 93090 Viroinval Viroinval 93000 Arrondissement Philippeville Arrondissement de Philippeville 90000.0 Provincie Namen Province de Namur 3000 Waals Gewest R\u00e9gion wallonne M BEL Belgen Belges 3 Weduwstaat Veuf 86 3 463379 93090 Viroinval Viroinval 93000 Arrondissement Philippeville Arrondissement de Philippeville 90000.0 Provincie Namen Province de Namur 3000 Waals Gewest R\u00e9gion wallonne M ETR niet-Belgen non-Belges 3 Weduwstaat Veuf 74 1 463380 93090 Viroinval Viroinval 93000 Arrondissement Philippeville Arrondissement de Philippeville 90000.0 Provincie Namen Province de Namur 3000 Waals Gewest R\u00e9gion wallonne M BEL Belgen Belges 3 Weduwstaat Veuf 52 1 <p>463381 rows \u00d7 21 columns</p> <pre><code>inhab_provence = df_inhab['TX_PROV_DESCR_NL'].dropna().unique()\ninhab_provence\n</code></pre> <pre><code>array(['Provincie Antwerpen', 'Provincie Vlaams-Brabant',\n       'Provincie Waals-Brabant', 'Provincie West-Vlaanderen',\n       'Provincie Oost-Vlaanderen', 'Provincie Henegouwen',\n       'Provincie Luik', 'Provincie Limburg', 'Provincie Luxemburg',\n       'Provincie Namen'], dtype=object)\n</code></pre> <pre><code>sc_provence = df_tot_sc['PROVINCE'].unique()\nsc_provence\n</code></pre> <pre><code>array(['Brussels', 'Li\u00e8ge', 'Limburg', 'OostVlaanderen', 'VlaamsBrabant',\n       'Antwerpen', 'WestVlaanderen', 'BrabantWallon', 'Hainaut', 'Namur',\n       nan, 'Luxembourg'], dtype=object)\n</code></pre> <pre><code>[p.split()[1] for p in inhab_provence]\n</code></pre> <pre><code>['Antwerpen',\n 'Vlaams-Brabant',\n 'Waals-Brabant',\n 'West-Vlaanderen',\n 'Oost-Vlaanderen',\n 'Henegouwen',\n 'Luik',\n 'Limburg',\n 'Luxemburg',\n 'Namen']\n</code></pre> <pre><code>map_statbel_provence_to_sc_provence = {'Provincie Antwerpen':'Antwerpen', 'Provincie Vlaams-Brabant':'VlaamsBrabant',\n       'Provincie Waals-Brabant':'BrabantWallon', 'Provincie West-Vlaanderen':'WestVlaanderen',\n       'Provincie Oost-Vlaanderen':'OostVlaanderen', 'Provincie Henegouwen':'Hainaut',\n       'Provincie Luik':'Li\u00e8ge', 'Provincie Limburg':'Limburg', 'Provincie Luxemburg':'Luxembourg',\n       'Provincie Namen':'Namur'}\n</code></pre> <pre><code>df_inhab['sc_provence'] = df_inhab['TX_PROV_DESCR_NL'].map(map_statbel_provence_to_sc_provence)\n</code></pre> <pre><code>df_tot_sc['AGEGROUP'].unique()\n</code></pre> <pre><code>array(['10-19', '20-29', '30-39', '40-49', '50-59', '70-79', '60-69',\n       '0-9', '90+', '80-89', nan], dtype=object)\n</code></pre> <pre><code>df_inhab['AGEGROUP'] =pd.cut(df_inhab['CD_AGE'], bins=[0,10,20,30,40,50,60,70,80,90,200], labels=['0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90+'], include_lowest=True)\n</code></pre> <pre><code>df_inhab_gender_prov = df_inhab.groupby(['sc_provence', 'CD_SEX', 'AGEGROUP'])['MS_POPULATION'].sum().reset_index()\n</code></pre> <pre><code>df_inhab_gender_prov_cases = pd.merge(df_inhab_gender_prov, df_tot_sc.dropna(), left_on=['sc_provence', 'AGEGROUP', 'CD_SEX'], right_on=['PROVINCE', 'AGEGROUP', 'SEX'])\n</code></pre> <pre><code>df_inhab_gender_prov_cases.head()\n</code></pre> sc_provence CD_SEX AGEGROUP MS_POPULATION DATE PROVINCE REGION SEX CASES 0 Antwerpen F 0-9 113851 2020-03-05 Antwerpen Flanders F 1 1 Antwerpen F 0-9 113851 2020-03-18 Antwerpen Flanders F 1 2 Antwerpen F 0-9 113851 2020-03-26 Antwerpen Flanders F 1 3 Antwerpen F 0-9 113851 2020-03-30 Antwerpen Flanders F 1 4 Antwerpen F 0-9 113851 2020-04-03 Antwerpen Flanders F 1 <pre><code>df_plot = df_inhab_gender_prov_cases.groupby(['SEX', 'AGEGROUP', 'PROVINCE']).agg(CASES = ('CASES', 'sum'), MS_POPULATION=('MS_POPULATION', 'first')).reset_index()\ndf_plot\n</code></pre> SEX AGEGROUP PROVINCE CASES MS_POPULATION 0 F 0-9 Antwerpen 9 113851 1 F 0-9 BrabantWallon 3 23744 2 F 0-9 Hainaut 11 81075 3 F 0-9 Limburg 11 48102 4 F 0-9 Li\u00e8ge 19 67479 ... ... ... ... ... ... 195 M 90+ Luxembourg 17 469 196 M 90+ Namur 27 827 197 M 90+ OostVlaanderen 102 3105 198 M 90+ VlaamsBrabant 129 2611 199 M 90+ WestVlaanderen 121 3292 <p>200 rows \u00d7 5 columns</p> <pre><code>df_plot['PROVINCE'].unique()\n</code></pre> <pre><code>array(['Antwerpen', 'BrabantWallon', 'Hainaut', 'Limburg', 'Li\u00e8ge',\n       'Luxembourg', 'Namur', 'OostVlaanderen', 'VlaamsBrabant',\n       'WestVlaanderen'], dtype=object)\n</code></pre> <pre><code>alt.Chart(df_plot).mark_bar().encode(x='AGEGROUP:N', y='CASES', color='SEX:N', column='PROVINCE:N')\n</code></pre> <pre><code>df_plot['percentage'] = df_plot['CASES'] / df_plot['MS_POPULATION']\n</code></pre> <pre><code>alt.Chart(df_plot).mark_bar().encode(x='AGEGROUP:N', y='percentage', color='SEX:N', column='PROVINCE:N')\n</code></pre> <p>Let's add a colorscale the makes the male blue and female number pink.</p> <pre><code>color_scale = alt.Scale(domain=['M', 'F'],\n                        range=['#1f77b4', '#e377c2'])\n</code></pre> <pre><code>alt.Chart(df_plot).mark_bar().encode(\n    x='AGEGROUP:N', \n    y='percentage', \n    color=alt.Color('SEX:N', scale=color_scale, legend=None),\n    column='PROVINCE:N')\n</code></pre> <p>The graph's get to wide. Let's use faceting to make two rows.</p> <p>Inspired and based on https://altair-viz.github.io/gallery/us_population_pyramid_over_time.html</p> <pre><code>#slider = alt.binding_range(min=1850, max=2000, step=10)\n# select_province = alt.selection_single(name='PROVINCE', fields=['PROVINCE'],\n#                                    bind=slider, init={'PROVINCE': 'Antwerpen'})\ncolor_scale = alt.Scale(domain=['Male', 'Female'],\n                        range=['#1f77b4', '#e377c2'])\n\nselect_province = alt.selection_multi(fields=['PROVINCE'], bind='legend')\n\nbase = alt.Chart(df_plot).add_selection(\n    select_province\n).transform_filter(\n    select_province\n).transform_calculate(\n    gender=alt.expr.if_(alt.datum.SEX == 'M', 'Male', 'Female')\n).properties(\n    width=250\n)\n\nleft = base.transform_filter(\n    alt.datum.gender == 'Female'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    x=alt.X('percentage:Q', axis=alt.Axis(format='.0%'),\n            title='Percentage',\n            sort=alt.SortOrder('descending'),\n            ),\n    color=alt.Color('gender:N', scale=color_scale, legend=None),\n).mark_bar().properties(title='Female')\n\nmiddle = base.encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    text=alt.Text('AGEGROUP:O'),\n).mark_text().properties(width=20)\n\nright = base.transform_filter(\n    alt.datum.gender == 'Male'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    x=alt.X('percentage:Q', title='Percentage', axis=alt.Axis(format='.0%'),),\n    color=alt.Color('gender:N', scale=color_scale, legend=None)\n).mark_bar().properties(title='Male')\n\n# legend = alt.Chart(df_plot).mark_text().encode(\n#     y=alt.Y('PROVINCE:N', axis=None),\n#     text=alt.Text('PROVINCE:N'),\n#     color=alt.Color('PROVINCE:N', legend=alt.Legend(title=\"Provincie\"))\n# )\n\nalt.concat(left, middle, right, spacing=5)\n\n#legend=alt.Legend(title=\"Species by color\")\n</code></pre> <pre><code>provinces = df_plot['PROVINCE'].unique()\nselect_province = alt.selection_single(\n    name='Select', # name the selection 'Select'\n    fields=['PROVINCE'], # limit selection to the Major_Genre field\n    init={'PROVINCE': 'Antwerpen'}, # use first genre entry as initial value\n    bind=alt.binding_select(options=provinces) # bind to a menu of unique provence values\n)\n\n\nbase = alt.Chart(df_plot).add_selection(\n    select_province\n).transform_filter(\n    select_province\n).transform_calculate(\n    gender=alt.expr.if_(alt.datum.SEX == 'M', 'Male', 'Female')\n).properties(\n    width=250\n)\n\nleft = base.transform_filter(\n    alt.datum.gender == 'Female'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    x=alt.X('percentage:Q', axis=alt.Axis(format='.0%'),\n            title='Percentage',\n            sort=alt.SortOrder('descending'),\n            scale=alt.Scale(domain=(0.0, 0.1), clamp=True)\n            ),\n    color=alt.Color('gender:N', scale=color_scale, legend=None),\n    tooltip=[alt.Tooltip('percentage', format='.1%')]\n).mark_bar().properties(title='Female')\n\nmiddle = base.encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    text=alt.Text('AGEGROUP:O'),\n).mark_text().properties(width=20)\n\nright = base.transform_filter(\n    alt.datum.gender == 'Male'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    x=alt.X('percentage:Q', title='Percentage', axis=alt.Axis(format='.1%'), scale=alt.Scale(domain=(0.0, 0.1), clamp=True)),\n    color=alt.Color('gender:N', scale=color_scale, legend=None),\n    tooltip=[alt.Tooltip('percentage', format='.1%')]\n).mark_bar().properties(title='Male')\n\nalt.concat(left, middle, right, spacing=5).properties(title='Percentage of covid-19 cases per province, gender and age grup in Belgium')\n</code></pre>"},{"location":"2020/04/22/regional-covid-19-mortality-in-belgium-per-gender-and-age/#mortality","title":"Mortality","text":"<pre><code># https://epistat.wiv-isp.be/covid/\n# Dataset of mortality by date, age, sex, and region\ndf_dead_sc = pd.read_csv('https://epistat.sciensano.be/Data/COVID19BE_MORT.csv')\n</code></pre> <pre><code>df_dead_sc.head()\n</code></pre> DATE REGION AGEGROUP SEX DEATHS 0 2020-03-10 Brussels 85+ F 1 1 2020-03-11 Flanders 85+ F 1 2 2020-03-11 Brussels 75-84 M 1 3 2020-03-11 Brussels 85+ F 1 4 2020-03-12 Brussels 75-84 M 1 <pre><code>df_dead_sc['REGION'].value_counts()\n</code></pre> <pre><code>Wallonia    291\nFlanders    275\nBrussels    271\nName: REGION, dtype: int64\n</code></pre> <pre><code>df_dead_sc['AGEGROUP'].value_counts()\n</code></pre> <pre><code>85+      223\n75-84    205\n65-74    179\n45-64    132\n25-44     19\n0-24       1\nName: AGEGROUP, dtype: int64\n</code></pre> <pre><code>df_inhab['AGEGROUP_sc'] =pd.cut(df_inhab['CD_AGE'], bins=[0,24,44,64,74,84,200], labels=['0-24','25-44','45-64','65-74','75-84','85+'], include_lowest=True)\n</code></pre> <pre><code>df_inhab.groupby('AGEGROUP_sc').agg(lowest_age=('CD_AGE', 'min'), highest_age=('CD_AGE', max))\n</code></pre> lowest_age highest_age AGEGROUP_sc 0-24 0 24 25-44 25 44 45-64 45 64 65-74 65 74 75-84 75 84 85+ 85 110 <pre><code>df_inhab.head()\n</code></pre> CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION sc_provence AGEGROUP AGEGROUP_sc 0 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 69 11 Antwerpen 60-69 65-74 1 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 80 3 Antwerpen 70-79 75-84 2 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande M BEL Belgen Belges 4 Gescheiden Divorc\u00e9 30 2 Antwerpen 20-29 25-44 3 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 48 26 Antwerpen 40-49 45-64 4 11001 Aartselaar Aartselaar 11000 Arrondissement Antwerpen Arrondissement d\u2019Anvers 10000.0 Provincie Antwerpen Province d\u2019Anvers 2000 Vlaams Gewest R\u00e9gion flamande F BEL Belgen Belges 4 Gescheiden Divorc\u00e9 76 2 Antwerpen 70-79 75-84 <pre><code>df_dead_sc['REGION'].unique()\n</code></pre> <pre><code>array(['Brussels', 'Flanders', 'Wallonia'], dtype=object)\n</code></pre> <pre><code>df_inhab['TX_RGN_DESCR_NL'].value_counts()\n</code></pre> <pre><code>Vlaams Gewest                     242865\nWaals Gewest                      199003\nBrussels Hoofdstedelijk Gewest     21513\nName: TX_RGN_DESCR_NL, dtype: int64\n</code></pre> <pre><code>df_inhab_gender_prov = df_inhab.groupby(['TX_RGN_DESCR_NL', 'CD_SEX', 'AGEGROUP_sc'])['MS_POPULATION'].sum().reset_index()\n</code></pre> <pre><code>region_sc_to_region_inhad = {'Flanders':'Vlaams Gewest', 'Wallonia':'Waals Gewest', 'Brussels':'Brussels Hoofdstedelijk Gewest'}\n</code></pre> <pre><code>df_dead_sc['TX_RGN_DESCR_NL'] = df_dead_sc['REGION'].map(region_sc_to_region_inhad)\n</code></pre> <pre><code>df_dead_sc.groupby(['TX_RGN_DESCR_NL', 'AGEGROUP', 'SEX'])['DEATHS'].sum()\n</code></pre> <pre><code>TX_RGN_DESCR_NL                 AGEGROUP  SEX\nBrussels Hoofdstedelijk Gewest  25-44     F        1\n                                          M        4\n                                45-64     F       21\n                                          M       43\n                                65-74     F       42\n                                          M       71\n                                75-84     F      128\n                                          M      170\n                                85+       F      270\n                                          M      186\nVlaams Gewest                   0-24      F        1\n                                25-44     F        2\n                                          M        3\n                                45-64     F       27\n                                          M       63\n                                65-74     F       67\n                                          M      130\n                                75-84     F      199\n                                          M      335\n                                85+       F      232\n                                          M      309\nWaals Gewest                    25-44     F        5\n                                          M        4\n                                45-64     F       41\n                                          M       89\n                                65-74     F       98\n                                          M      186\n                                75-84     F      290\n                                          M      300\n                                85+       F      704\n                                          M      421\nName: DEATHS, dtype: int64\n</code></pre> <pre><code>df_dead_sc_region_agegroup_gender = df_dead_sc.groupby(['TX_RGN_DESCR_NL', 'AGEGROUP', 'SEX'])['DEATHS'].sum().reset_index()\n</code></pre> <pre><code>df_inhab_gender_prov_deaths = pd.merge(df_inhab_gender_prov, df_dead_sc_region_agegroup_gender, left_on=['TX_RGN_DESCR_NL', 'AGEGROUP_sc', 'CD_SEX'], right_on=['TX_RGN_DESCR_NL', 'AGEGROUP', 'SEX'])\n</code></pre> <pre><code>df_inhab_gender_prov_deaths['MS_POPULATION'].sum()\n</code></pre> <pre><code>9077403\n</code></pre> <pre><code>df_inhab_gender_prov_deaths['DEATHS'].sum()\n</code></pre> <pre><code>4442\n</code></pre> <pre><code>df_inhab_gender_prov_deaths\n</code></pre> TX_RGN_DESCR_NL CD_SEX AGEGROUP_sc MS_POPULATION AGEGROUP SEX DEATHS 0 Brussels Hoofdstedelijk Gewest F 25-44 197579 25-44 F 1 1 Brussels Hoofdstedelijk Gewest F 45-64 137628 45-64 F 21 2 Brussels Hoofdstedelijk Gewest F 65-74 45214 65-74 F 42 3 Brussels Hoofdstedelijk Gewest F 75-84 30059 75-84 F 128 4 Brussels Hoofdstedelijk Gewest F 85+ 18811 85+ F 270 5 Brussels Hoofdstedelijk Gewest M 25-44 194988 25-44 M 4 6 Brussels Hoofdstedelijk Gewest M 45-64 140348 45-64 M 43 7 Brussels Hoofdstedelijk Gewest M 65-74 36698 65-74 M 71 8 Brussels Hoofdstedelijk Gewest M 75-84 19969 75-84 M 170 9 Brussels Hoofdstedelijk Gewest M 85+ 7918 85+ M 186 10 Vlaams Gewest F 0-24 874891 0-24 F 1 11 Vlaams Gewest F 25-44 820036 25-44 F 2 12 Vlaams Gewest F 45-64 901554 45-64 F 27 13 Vlaams Gewest F 65-74 353925 65-74 F 67 14 Vlaams Gewest F 75-84 245981 75-84 F 199 15 Vlaams Gewest F 85+ 132649 85+ F 232 16 Vlaams Gewest M 25-44 827281 25-44 M 3 17 Vlaams Gewest M 45-64 917008 45-64 M 63 18 Vlaams Gewest M 65-74 336242 65-74 M 130 19 Vlaams Gewest M 75-84 193576 75-84 M 335 20 Vlaams Gewest M 85+ 69678 85+ M 309 21 Waals Gewest F 25-44 457356 25-44 F 5 22 Waals Gewest F 45-64 496668 45-64 F 41 23 Waals Gewest F 65-74 199422 65-74 F 98 24 Waals Gewest F 75-84 118224 75-84 F 290 25 Waals Gewest F 85+ 68502 85+ F 704 26 Waals Gewest M 25-44 459444 25-44 M 4 27 Waals Gewest M 45-64 487322 45-64 M 89 28 Waals Gewest M 65-74 175508 65-74 M 186 29 Waals Gewest M 75-84 82876 75-84 M 300 30 Waals Gewest M 85+ 30048 85+ M 421 <pre><code>df_inhab_gender_prov_deaths['percentage'] = df_inhab_gender_prov_deaths['DEATHS']/df_inhab_gender_prov_deaths['MS_POPULATION']\n</code></pre> <pre><code>df_plot = df_inhab_gender_prov_deaths\n</code></pre> <pre><code>regions = df_plot['TX_RGN_DESCR_NL'].unique()\nselect_province = alt.selection_single(\n    name='Select', # name the selection 'Select'\n    fields=['TX_RGN_DESCR_NL'], # limit selection to the Major_Genre field\n    init={'TX_RGN_DESCR_NL': 'Vlaams Gewest'}, # use first genre entry as initial value\n    bind=alt.binding_select(options=regions) # bind to a menu of unique provence values\n)\n\nbase = alt.Chart(df_plot).add_selection(\n    select_province\n).transform_filter(\n    select_province\n).transform_calculate(\n    gender=alt.expr.if_(alt.datum.SEX == 'M', 'Male', 'Female')\n).properties(\n    width=250\n)\n\nleft = base.transform_filter(\n    alt.datum.gender == 'Female'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    x=alt.X('percentage:Q', axis=alt.Axis(format='.2%'),\n            title='Percentage',\n            sort=alt.SortOrder('descending'),\n            # scale=alt.Scale(domain=(0.0, 0.02), clamp=True)\n            ),\n    color=alt.Color('gender:N', scale=color_scale, legend=None),\n    tooltip=[alt.Tooltip('percentage', format='.2%')]\n).mark_bar().properties(title='Female')\n\nmiddle = base.encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    text=alt.Text('AGEGROUP:O'),\n).mark_text().properties(width=20)\n\nright = base.transform_filter(\n    alt.datum.gender == 'Male'\n).encode(\n    y=alt.Y('AGEGROUP:O', axis=None),\n    # x=alt.X('percentage:Q', title='Percentage', axis=alt.Axis(format='.2%'), scale=alt.Scale(domain=(0.0, 0.02), clamp=True)),\n    x=alt.X('percentage:Q', title='Percentage', axis=alt.Axis(format='.2%')),\n    color=alt.Color('gender:N', scale=color_scale, legend=None),\n    tooltip=[alt.Tooltip('percentage', format='.2%')]\n).mark_bar().properties(title='Male')\n\nalt.concat(left, middle, right, spacing=5).properties(title='Percentage of covid-19 deaths per province, gender and age group relative to number of inhabitants in Belgium')\n</code></pre> <pre><code>\n</code></pre>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/","title":"Explain clusters to business with Altair and Shapley values","text":"<p>Everyone that calls herselve a data scientist has at least clustered the Iris dataset. This notebook goes beyond the classical dimension reduction and clustering. I gives you two extra superpowerS to explain the resulting clusters to your client.  - First: I'll show how we can add extra insight by using interactive charts made with interactive graphs in Altair. - Second I explain and show how to derive important clues from Shap values by training a classification model on the clustering result. With the Altair and shap libraries in your datascience toolbelt, you will excel at the clustering tasks thrown at you.</p> <ul> <li>image: pca_cluster_altair_view.png</li> </ul> <p>The data is a set of about 5000 wines from the UCI Machine learning Repository. The dataset is related to the white variants of the Portuguese \"Vinho Verde\" wine. There are 11 features that describe chemical characteristics of the wines. The task is to find clusters of wine that are in a way similar to each other. By reducing the 5000 wines to a small number of groups of wine, you can give strategic advise to your business by generalising per cluster instead of per wine.</p> <p>We are not using the quality of the wine. This notebook is to illustrate a unsupervised clustering approach. We start from the classic PCA dimension reduction for visualisation and Kmeans for clustering. Other dimension reduction techniques (LDA, UMAP, t-sne, ivis, ...) and clustering techniques (density based, aglomerative, gaussian mixture, ...) are left as an excercise for the reader.</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#references","title":"References","text":"<p>Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.</p> <pre><code># Load python libraries for data handling and plotting\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code># Load the data set with wines from the internet\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')\n</code></pre> <pre><code># Show the first lines of the data set to get an idea what's in there.\ndf.head()\n</code></pre> fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 7.0 0.27 0.36 20.7 0.045 45.0 170.0 1.0010 3.00 0.45 8.8 6 1 6.3 0.30 0.34 1.6 0.049 14.0 132.0 0.9940 3.30 0.49 9.5 6 2 8.1 0.28 0.40 6.9 0.050 30.0 97.0 0.9951 3.26 0.44 10.1 6 3 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 4 7.2 0.23 0.32 8.5 0.058 47.0 186.0 0.9956 3.19 0.40 9.9 6 <pre><code># How many wines and features do we have ?\ndf.shape\n</code></pre> <pre><code>(4898, 12)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol', 'quality'],\n      dtype='object')\n</code></pre> <pre><code># Define the standard X (feature matrix) and target series y (not used in here)\nX =  df.drop(columns='quality')\nall_features = X.columns\ny = df['quality']\n</code></pre> <pre><code>df.describe()\n</code></pre> fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality count 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 4898.000000 mean 6.854788 0.278241 0.334192 6.391415 0.045772 35.308085 138.360657 0.994027 3.188267 0.489847 10.514267 5.877909 std 0.843868 0.100795 0.121020 5.072058 0.021848 17.007137 42.498065 0.002991 0.151001 0.114126 1.230621 0.885639 min 3.800000 0.080000 0.000000 0.600000 0.009000 2.000000 9.000000 0.987110 2.720000 0.220000 8.000000 3.000000 25% 6.300000 0.210000 0.270000 1.700000 0.036000 23.000000 108.000000 0.991723 3.090000 0.410000 9.500000 5.000000 50% 6.800000 0.260000 0.320000 5.200000 0.043000 34.000000 134.000000 0.993740 3.180000 0.470000 10.400000 6.000000 75% 7.300000 0.320000 0.390000 9.900000 0.050000 46.000000 167.000000 0.996100 3.280000 0.550000 11.400000 6.000000 max 14.200000 1.100000 1.660000 65.800000 0.346000 289.000000 440.000000 1.038980 3.820000 1.080000 14.200000 9.000000 <pre><code>alt.Chart(df).mark_point().encode(x='fixed acidity', y='volatile acidity', color='quality:N')\n</code></pre> <pre><code># Add interactivity\nalt.Chart(df).mark_point().encode(x='fixed acidity', y='volatile acidity', color='quality:N', tooltip=['alcohol', 'quality']).interactive()\n</code></pre>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#scaling","title":"Scaling","text":"<p>From the describe, we see that domains of the feature differ widely. Feature 'fixed acidity' has mean 6.854788, while feature 'volatile acidity' has mean 0.278241. This is a sign that are on a different scale. We scale the features first so that we can use them together.</p> <pre><code># When features have different scale we have to scale them so that we can use them together\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code>scaler = StandardScaler() \n# scaler = RobustScaler()  # Take the robust scaler when data contains outliers that you want to remove\n</code></pre> <pre><code>X_scaled = scaler.fit_transform(X)\n</code></pre>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#pca","title":"PCA","text":"<p>Perform principal component analysis to get an idea of the dimensionality of the wine dataset. Next reduce to two dimension so that we can make a 2D scatter plot were each dot is a wine from the set.</p> <pre><code>from sklearn.decomposition import PCA\npca = PCA().fit(X_scaled)\n</code></pre> <pre><code>df_plot = pd.DataFrame({'Component Number': 1+np.arange(X.shape[1]),\n                        'Cumulative explained variance': np.cumsum(pca.explained_variance_ratio_)})\nalt.Chart(df_plot).mark_bar().encode(\n    x=alt.X('Component Number:N', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('Cumulative explained variance', axis=alt.Axis(format='%', title='Percentage')),\n    tooltip=[alt.Tooltip('Cumulative explained variance', format='.0%')]\n).properties(\n    title='Cumulative explained variance'\n).properties(\n    width=400\n)\n</code></pre> <p>We see that almost all features are needed to explain all the variance. Only with 9 comoponent we can explain 97% of the variance. This means that we should use all features from the dataset and feature selection is not necessary in this case.</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#pca-with-2-dimensions","title":"PCA with 2 Dimensions","text":"<p>Perform dimension reduction to two dimentions with PCA to be able to draw all the wines in one graph (not for the clustering!)</p> <pre><code>twod_pca = PCA(n_components=2)\nX_pca = twod_pca.fit_transform(X_scaled)\n</code></pre> <pre><code># Add first to PCA components to the dataset so we can plot it\ndf['pca1'] = X_pca[:,0]\ndf['pca2'] = X_pca[:,1]\n</code></pre> <pre><code>df['member'] = 1\ndf.groupby('quality')['member'].transform('count').div(df.shape[0])\n</code></pre> <pre><code>0       0.448755\n1       0.448755\n2       0.448755\n3       0.448755\n4       0.448755\n          ...   \n4893    0.448755\n4894    0.297468\n4895    0.448755\n4896    0.179665\n4897    0.448755\nName: member, Length: 4898, dtype: float64\n</code></pre> <pre><code>selection = alt.selection_multi(fields=['quality'], bind='legend')\n\ndf['quality_weight'] = df['quality'].map(df['quality'].value_counts(normalize=True).to_dict())\n# Draw 20% stratified sample\nalt.Chart(df.sample(1000, weights='quality_weight')).mark_circle(size=60).encode(\n    x=alt.X('pca1', title='First component'),\n    y=alt.Y('pca2', title='Second component'),\n    color=alt.Color('quality:N'),\n    tooltip=['quality'],\n    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n).properties(\n    title='PCA analyse',\n    width=600,\n    height=400\n).add_selection(\n    selection\n)\n</code></pre> <pre><code>df_twod_pca = pd.DataFrame(data=twod_pca.components_.T, columns=['pca1', 'pca2'], index=X.columns)\n</code></pre> <pre><code>pca1 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode(\n    y=alt.Y('index:O', title=None),\n    x='pca1',\n    color=alt.Color('pca1', scale=alt.Scale(scheme='viridis')),\n    tooltip = [alt.Tooltip('index', title='Feature'), alt.Tooltip('pca1', format='.2f')]\n)\npca2 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode(\n    y=alt.Y('index:O', title=None),\n    x='pca2',\n    color=alt.Color('pca2', scale=alt.Scale(scheme='viridis')),\n    tooltip = [alt.Tooltip('index', title='Feature'), alt.Tooltip('pca2', format='.2f')]\n)\n\n(pca1 &amp; pca2).properties(\n    title='Loadings of the first two principal components'\n)\n</code></pre>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#kmeans-clustering","title":"Kmeans clustering","text":"<p>Determine the number of cluster for Kmeans clustering by lopping from 2 to 11 cluster. Calculate for each result the within-cluster variation (inertia), the silhoutte and the Davies-Bouldin index. Use the elbow method to determine the number of clusters. The elbow method seeks the value of k after which the clustering quality improves only marginally.</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n</code></pre> <pre><code>km_scores= []\nkm_silhouette = []\nkm_db_score = []\nfor i in range(2,X.shape[1]):\n    km = KMeans(n_clusters=i, random_state=1301).fit(X_scaled)\n    preds = km.predict(X_scaled)\n\n    print(f'Score for number of cluster(s) {i}: {km.score(X_scaled):.3f}')\n    km_scores.append(-km.score(X_scaled))\n\n    silhouette = silhouette_score(X_scaled,preds)\n    km_silhouette.append(silhouette)\n    print(f'Silhouette score for number of cluster(s) {i}: {silhouette:.3f}')\n\n    db = davies_bouldin_score(X_scaled,preds)\n    km_db_score.append(db)\n    print(f'Davies Bouldin score for number of cluster(s) {i}: {db:.3f}')\n\n    print('-'*100)\n</code></pre> <pre><code>Score for number of cluster(s) 2: -42548.672\nSilhouette score for number of cluster(s) 2: 0.214\nDavies Bouldin score for number of cluster(s) 2: 1.775\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 3: -39063.495\nSilhouette score for number of cluster(s) 3: 0.144\nDavies Bouldin score for number of cluster(s) 3: 2.097\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 4: -35986.918\nSilhouette score for number of cluster(s) 4: 0.159\nDavies Bouldin score for number of cluster(s) 4: 1.809\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 5: -33699.043\nSilhouette score for number of cluster(s) 5: 0.144\nDavies Bouldin score for number of cluster(s) 5: 1.768\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 6: -31973.251\nSilhouette score for number of cluster(s) 6: 0.146\nDavies Bouldin score for number of cluster(s) 6: 1.693\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 7: -30552.998\nSilhouette score for number of cluster(s) 7: 0.126\nDavies Bouldin score for number of cluster(s) 7: 1.847\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 8: -29361.568\nSilhouette score for number of cluster(s) 8: 0.128\nDavies Bouldin score for number of cluster(s) 8: 1.790\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 9: -28198.302\nSilhouette score for number of cluster(s) 9: 0.128\nDavies Bouldin score for number of cluster(s) 9: 1.760\n----------------------------------------------------------------------------------------------------\nScore for number of cluster(s) 10: -27444.897\nSilhouette score for number of cluster(s) 10: 0.118\nDavies Bouldin score for number of cluster(s) 10: 1.843\n----------------------------------------------------------------------------------------------------\n</code></pre> <pre><code>df_plot = pd.DataFrame({'Number of clusters':[i for i in range(2,X.shape[1])],'kmean score': km_scores})\nalt.Chart(df_plot).mark_line(point=True).encode(\n    x=alt.X('Number of clusters:N', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('kmean score'),\n    tooltip=[alt.Tooltip('kmean score', format='.2f')]\n).properties(\n    title='Kmean score ifo number of cluster'\n).properties(\n    title='The elbow method for determining number of clusters',\n    width=400\n)\n</code></pre> <pre><code>df_plot = pd.DataFrame({'Number of clusters':[i for i in range(2,X.shape[1])],'silhouette score': km_silhouette})\nalt.Chart(df_plot).mark_line(point=True).encode(\n    x=alt.X('Number of clusters:N', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('silhouette score'),\n    tooltip=[alt.Tooltip('silhouette score', format='.2f')]\n).properties(\n    title='Silhouette  score ifo number of cluster',\n    width=400\n)\n</code></pre> <pre><code>df_plot = pd.DataFrame({'Number of clusters':[i for i in range(2, X.shape[1])],'davies bouldin score': km_db_score})\nalt.Chart(df_plot).mark_line(point=True).encode(\n    x=alt.X('Number of clusters:N', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('davies bouldin score'),\n    tooltip=[alt.Tooltip('davies bouldin score', format='.2f')]\n).properties(\n    title='Davies Bouldin score ifo number of cluster',\n    width=400\n)\n</code></pre> <pre><code>from sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\nX = X.values\nrange_n_clusters = [2, 3, 4, 5, 6]\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()\n</code></pre> <pre><code>For n_clusters = 2 The average silhouette_score is : 0.5062782327345698\nFor n_clusters = 3 The average silhouette_score is : 0.4125412743885912\nFor n_clusters = 4 The average silhouette_score is : 0.3748324919186734\nFor n_clusters = 5 The average silhouette_score is : 0.3437053439455249\nFor n_clusters = 6 The average silhouette_score is : 0.313821767576001\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#make-three-clusters","title":"Make three clusters","text":"<p>Since we have a high Davies Boldin and low Silhoutte score for k=3 we select to cluster into three clusters. Another option could be to use the Gaussian Likelihood score. In this notebook another analysis reported also 3 clusters.</p> <pre><code>km = KMeans(n_clusters=3, random_state=1301).fit(X_scaled)\npreds = km.predict(X_scaled)\npd.Series(preds).value_counts() # How many wines are in each cluster ?\n</code></pre> <pre><code>0    1801\n1    1629\n2    1468\ndtype: int64\n</code></pre> <pre><code>df_km = pd.DataFrame(data={'pca1':X_pca[:,0], 'pca2':X_pca [:,1], 'cluster':preds})\n</code></pre> <pre><code># Add the scaled data with the input features\n\nfor i,c in enumerate(all_features):\n    df_km[c] = X_scaled[:,i]\n</code></pre> <pre><code>domain = [0, 1, 2]\nrange_ = ['red',  'darkblue', 'green']\n\nselection = alt.selection_multi(fields=['cluster'], bind='legend')\n\npca = alt.Chart(df_km).mark_circle(size=20).encode(\n    x='pca1',\n    y='pca2',\n    color=alt.Color('cluster:N', scale=alt.Scale(domain=domain, range=range_)),\n    opacity=alt.condition(selection, alt.value(1), alt.value(0.1)),\n    tooltip = list(all_features)\n).add_selection(\n    selection\n)\n\npca\n</code></pre> <p>Now we will plot the all the wines on a two dimensional plane. On the right, you get boxplots for every feature. The cool thing is now that with the mouse you can select by drawing a brush over the points (click, hold button and drag the mouse) and get immediate updates boxplots of the features of the selected wines. A such you can interactively gain some insight in what the cluster might mean.</p> <pre><code>brush = alt.selection(type='interval')\n\ndomain = [0, 1, 2]\nrange_ = ['red', 'darkblue', 'green']\n\npoints = alt.Chart(df_km).mark_circle(size=60).encode(\n    x='pca1',\n    y='pca2',\n    color = alt.condition(brush, 'cluster:N', alt.value('lightgray')),\n    tooltip = list(all_features)\n).add_selection(brush)\n\nboxplots = alt.vconcat()\nfor measure in all_features:\n    boxplot = alt.Chart(df_km).mark_boxplot().encode(\n            x =alt.X(measure, axis=alt.Axis(titleX=470, titleY=0)),\n    ).transform_filter(\n            brush\n    )\n    boxplots &amp;= boxplot\n\nchart = alt.hconcat(points, boxplots)\n# chart.save('cluster_pca_n_3.html'))\nchart\n</code></pre> <p>AS you can't select al wine from one cluster with the rectangular brush, we make a plot for each cluster and boxplots of the features values of that cluster.</p> <pre><code>def plot_cluster(df, selected_columns, clusternr):\n    points = alt.Chart(df).mark_circle(size=60).encode(\n    x=alt.X('pca1', title='Principal component 1 (pca1)'),\n    y=alt.Y('pca2', title='Principal component 2 (pca1)'),\n    color = alt.condition(alt.FieldEqualPredicate(field='cluster', equal=clusternr), 'cluster:N', alt.value('lightgray')),\n    tooltip = list(all_features)+['cluster'],\n)\n\n    boxplots = alt.vconcat()\n    for measure in  [c for c in selected_columns]:\n        boxplot = alt.Chart(df).mark_boxplot().encode(\n                x =alt.X(measure, axis=alt.Axis(titleX=480, titleY=0)),\n        ).transform_filter(\n                alt.FieldEqualPredicate(field='cluster', equal=clusternr)\n        )\n        boxplots &amp;= boxplot\n    return points, boxplots\n</code></pre> <pre><code>points, boxplots = plot_cluster(df_km, all_features, 0)\nc0 = alt.hconcat(points, boxplots).properties(title='Cluster 0')\npoints, boxplots = plot_cluster(df_km, all_features, 1)\nc1 = alt.hconcat(points, boxplots).properties(title='Cluster 1')\npoints, boxplots = plot_cluster(df_km, all_features, 2)\nc2 = alt.hconcat(points, boxplots).properties(title='Cluster 2')\n</code></pre> <pre><code>alt.vconcat(c0, c1, c2)\n</code></pre> <p>I might still be difficult to explain the clusters. We will now build a multiclass classifier to predict the cluster from the features. Next we will use the Shapley values to explain the clusters.</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#light-gbm-classifier","title":"Light gbm classifier","text":"<pre><code>import lightgbm as lgb\n</code></pre> <pre><code>params = lgb.LGBMClassifier().get_params()\nparams\n</code></pre> <pre><code>{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 1.0,\n 'importance_type': 'split',\n 'learning_rate': 0.1,\n 'max_depth': -1,\n 'min_child_samples': 20,\n 'min_child_weight': 0.001,\n 'min_split_gain': 0.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_leaves': 31,\n 'objective': None,\n 'random_state': None,\n 'reg_alpha': 0.0,\n 'reg_lambda': 0.0,\n 'silent': True,\n 'subsample': 1.0,\n 'subsample_for_bin': 200000,\n 'subsample_freq': 0}\n</code></pre> <pre><code>params['objective'] = 'multiclass' # the target to predict is the number of the cluster\nparams['is_unbalance'] = True\nparams['n_jobs'] = -1\nparams['random_state'] = 1301\n</code></pre> <pre><code>mdl = lgb.LGBMClassifier(**params)\n</code></pre> <pre><code>X =  df.drop(columns=['quality', 'pca1', 'pca2', 'member'])\n</code></pre> <pre><code>mdl.fit(X, preds)\n</code></pre> <pre><code>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', is_unbalance=True, learning_rate=0.1,\n               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n               objective='multiclass', random_state=1301, reg_alpha=0.0,\n               reg_lambda=0.0, silent=True, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)\n</code></pre> <pre><code>y_pred = mdl.predict_proba(X)\n</code></pre> <pre><code># Install the shap library to caculate the Shapley values\n!pip install shap\n</code></pre> <pre><code>Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.2)\nRequirement already satisfied: tqdm&gt;4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.38.0)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2018.9)\nRequirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2.8.1)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;shap) (0.14.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.6.1-&gt;pandas-&gt;shap) (1.12.0)\n</code></pre> <pre><code>import shap\n</code></pre> <pre><code>explainer = shap.TreeExplainer(mdl)\nshap_values = explainer.shap_values(X)\n</code></pre> <pre><code>Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n</code></pre> <pre><code>shap.summary_plot(shap_values, X, max_display=30)\n</code></pre> <p>From the Shapley values we see that the feature density has the highest impact on the model to predict the clusters. Let's have a look the Shapley values per cluster. The acidity features pH and fixed acidity has only impact on cluster 1 and 2 but almost none on cluster 0.</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#shapley-values-for-the-three-clusters","title":"Shapley values for the three clusters","text":"<pre><code>for cnr in df_km['cluster'].unique():\n    shap.summary_plot(shap_values[cnr], X, max_display=30, show=False)\n    plt.title(f'Cluster {cnr}')\n    plt.show()\n</code></pre> <p>To explain the clusters, we will plot the three clusters and the boxplot of the features ordered with the feature importance.</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#explain-cluster-0","title":"Explain cluster 0","text":"<pre><code>cnr = 0\nfeature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0))\npoints, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], 0)\nc0 = alt.hconcat(points, boxplots).properties(title='Cluster 0')\nc0\n</code></pre> <pre><code>cnr = 0\nshap.summary_plot(shap_values[cnr], X, max_display=30, show=False)\nplt.title(f'Cluster {cnr}')\nplt.show()\n</code></pre> <p>Cluster 0 can be describe as wines with: - high density - high total sulfur dioxide - high free sulfur dioxide</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#explain-cluster-1","title":"Explain cluster 1","text":"<pre><code>X.columns\n</code></pre> <pre><code>Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol', 'quality_weight'],\n      dtype='object')\n</code></pre> <pre><code>cnr = 1\nfeature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0))\npoints, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr)\nc1 = alt.hconcat(points, boxplots).properties(title=f'Cluster {cnr}')\nc1\n</code></pre> <pre><code>cnr = 1\nshap.summary_plot(shap_values[cnr], X, max_display=30, show=False)\nplt.title(f'Cluster {cnr}')\nplt.show()\n</code></pre> <p>Cluster 1 contains wines with: - high pH - low fixed acidity - low density (opposite of cluster 0) - low citric acid - high on sulphates</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#explain-cluster-2","title":"Explain cluster 2","text":"<pre><code>cnr = 2\nfeature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0))\npoints, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr)\nc2 = alt.hconcat(points, boxplots).properties(title=f'Cluster {cnr}')\nc2\n</code></pre> <pre><code>cnr = 2\nshap.summary_plot(shap_values[cnr], X, max_display=30, show=False)\nplt.title(f'Cluster {cnr}')\nplt.show()\n</code></pre> <p>Cluster 2 contains wines with: - high fixed acidity - low pH (opposite of cluster 1) - low density (opposite of cluster 0)</p>"},{"location":"2020/04/23/explain-clusters-to-business-with-altair-and-shapley-values/#how-is-the-quality-of-the-whines-in-each-cluster","title":"How is the quality of the whines in each cluster ?","text":"<pre><code>df_km['quality'] = y\ndf_km.groupby('cluster')['quality'].describe()\n</code></pre> count mean std min 25% 50% 75% max cluster 0 1801.0 5.606330 0.750514 3.0 5.0 6.0 6.0 8.0 1 1629.0 6.150399 0.905265 3.0 6.0 6.0 7.0 9.0 2 1468.0 5.908719 0.918554 3.0 5.0 6.0 6.0 9.0 <pre><code># Let's plot the distributions of quality score per cluster\nalt.Chart(df_km).transform_density(\n    density='quality',\n    bandwidth=0.3,\n    groupby=['cluster'],\n    extent= [3, 9],\n    counts = True,\n    steps=200\n).mark_area().encode(\n    alt.X('value:Q'),\n    alt.Y('density:Q', stack='zero'),\n    alt.Color('cluster:N')\n).properties(\n    title='Distribution of quality of the wines per cluster',\n    width=400,\n    height=100\n)\n</code></pre> <p>We see that the quality has a similar distribution in each cluster. </p> <pre><code>\n</code></pre>"},{"location":"2020/04/24/my-talk-at-data-science-leuven/","title":"My talk at data science leuven","text":"<p>Links to video and slides of the talk and thanking people.</p>"},{"location":"2020/04/24/my-talk-at-data-science-leuven/#talk-material","title":"Talk material","text":"<p>On 23 April 2020, I was invited for a talk at Data science Leuven. I talked about how you can explore and explain the results of a clustering exercise. The target audience are data scientists that that have notions of how to cluster data and that want to improve their skills. </p> <p>The video is recorded on Youtube:</p> <p>youtube: https://youtu.be/hk0arqhcX9U?t=3570</p> <p>You can see the slides here: slides</p> <p>The talk itself is based on this notebook that I published on this blog yesterday and that I used to demo during the talk.</p> <p>The host of the conference was Istvan Hajnal. He tweeted the following: </p> <p>twitter: https://twitter.com/dsleuven/status/1253391470444371968</p> <p>He also took the R out of my family name NachteRgaele. Troubles with R, it's becoming a story of my life...  Behind the scene Kris Peeters calmly took the heat of doing the live streaming.  Almost Pydata quality! Big thanks to the whole Data Science Leuven team that is doing all this on voluntary basis.</p>"},{"location":"2020/04/24/my-talk-at-data-science-leuven/#standing-on-the-shoulders-of-the-giants","title":"Standing on the shoulders of the giants","text":"<p>This talks was not possible without the awesome Altair visualisation library made by Jake VanderPlas. Secondly, it builds upon the open source Shap library made by Scott Lundberg. Those two libraries had a major impact on my daily work as datascientist at Colruyt group. They inspired me in trying to give back to the open source community with this talk. </p> <p>If you want to learn how to use Altair I recommend the tutorial made by Vincent Warmerdam on his calm code site: https://calmcode.io/altair/introduction.htm</p> <p>I would also like to thank my collegues at work who endured the dry-run of this talk and who made the suggestion to try to use a classifier to explain the clustering result. Top team!</p>"},{"location":"2020/04/24/my-talk-at-data-science-leuven/#awesome-fastpages","title":"Awesome fastpages","text":"<p>Finally, this blog is build with the awesome fastpages. I can now share a rendered Jupyter notebook, with working interactive demos, that can be opened in My binder or Google Colab with one click on a button. This means that readers can directly tinker around with the code and methods discussed in the talk. All you need is a browser and an internet connection. So thank you Jeremy Howard, Hamel Husain, and the fastdotai team for pulling this off.  Thank you Hamel Husain for your Github Actions. I will cast for two how awesome this all is.</p>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/","title":"Estimating the effective reproduction number in Belgium","text":"<p>Applying the Bayesian model from Rt.live on Belgium test data.</p> <p>In this post we estimate the effective reproduction number of COVID-19 in the northern and southern part of Belgium. We apply the Bayesian model of rt.live on Belgian data of COVID-19 tests provided by the goverment.</p>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/#install-needed-packages-and-software","title":"Install needed packages and software","text":"<pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <p>The current version of pymc3, installed by default in Colab, is version 3.3.7. The requirements for the Bayesian model of rt.live stipulates a more recent version. We first uninstall verions 3.3.7 and then install a version v3.9.3.</p> <pre><code>!pip  uninstall -y pymc3\n</code></pre> <pre><code>Uninstalling pymc3-3.7:\n  Successfully uninstalled pymc3-3.7\n</code></pre> <pre><code>!pip install pymc3&gt;=3.9.2\n!pip install arviz&gt;=0.9.0\n</code></pre> <pre><code>import warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\nimport pymc3 as pm\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as sps\n\nimport theano\nimport theano.tensor as tt\nfrom theano.tensor.signal.conv import conv2d\n\nimport seaborn as sns\nsns.set_context('talk')\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\n</code></pre> <pre><code>print('Running on PyMC3 v{}'.format(pm.__version__))\n</code></pre> <pre><code>Running on PyMC3 v3.9.3\n</code></pre> <p>Now that we are running a recent version of pymc3, we can install the model:</p> <p><code>!pip install git+https://github.com/rtcovidlive/covid-model.git</code></p> <p>Unfortunately, this does not work. I pasted the code in this notebook as a workaround.</p> <pre><code>#hide\n\n# from covid.patients import get_delay_distribution\n# p_delay = pd.read_csv('https://raw.githubusercontent.com/rtcovidlive/covid-model/master/data/p_delay.csv')\n# From https://github.com/rtcovidlive/covid-model/blob/master/covid/models/generative.py\n</code></pre> <pre><code>#collapse\n\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\nimport pymc3 as pm\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as sps\n\nimport theano\nimport theano.tensor as tt\nfrom theano.tensor.signal.conv import conv2d\n\n# from covid.patients import get_delay_distribution\n\n\nclass GenerativeModel:\n    version = \"1.0.0\"\n\n    def __init__(self, region: str, observed: pd.DataFrame, buffer_days=10):\n        \"\"\" Takes a region (ie State) name and observed new positive and\n            total test counts per day. buffer_days is the default number of\n            blank days we pad on the leading edge of the time series because\n            infections occur long before reports and we need to infer values\n            on those days \"\"\"\n\n        first_index = observed.positive.ne(0).argmax()\n        observed = observed.iloc[first_index:]\n        new_index = pd.date_range(\n            start=observed.index[0] - pd.Timedelta(days=buffer_days),\n            end=observed.index[-1],\n            freq=\"D\",\n        )\n        observed = observed.reindex(new_index, fill_value=0)\n\n        self._trace = None\n        self._inference_data = None\n        self.model = None\n        self.observed = observed\n        self.region = region\n\n    @property\n    def n_divergences(self):\n        \"\"\" Returns the number of divergences from the current trace \"\"\"\n        assert self.trace != None, \"Must run sample() first!\"\n        return self.trace[\"diverging\"].nonzero()[0].size\n\n    @property\n    def inference_data(self):\n        \"\"\" Returns an Arviz InferenceData object \"\"\"\n        assert self.trace, \"Must run sample() first!\"\n\n        with self.model:\n            posterior_predictive = pm.sample_posterior_predictive(self.trace)\n\n        _inference_data = az.from_pymc3(\n            trace=self.trace,\n            posterior_predictive=posterior_predictive,\n        )\n        _inference_data.posterior.attrs[\"model_version\"] = self.version\n\n        return _inference_data\n\n    @property\n    def trace(self):\n        \"\"\" Returns the trace from a sample() call. \"\"\"\n        assert self._trace, \"Must run sample() first!\"\n        return self._trace\n\n    def _scale_to_positives(self, data):\n        \"\"\" Scales a time series to have the same mean as the observed positives\n            time series. This is useful because many of the series we infer are\n            relative to their true values so we make them comparable by putting\n            them on the same scale. \"\"\"\n        scale_factor = self.observed.positive.mean() / np.mean(data)\n        return scale_factor * data\n\n    def _get_generation_time_interval(self):\n        \"\"\" Create a discrete P(Generation Interval)\n            Source: https://www.ijidonline.com/article/S1201-9712(20)30119-3/pdf \"\"\"\n        mean_si = 4.7\n        std_si = 2.9\n        mu_si = np.log(mean_si ** 2 / np.sqrt(std_si ** 2 + mean_si ** 2))\n        sigma_si = np.sqrt(np.log(std_si ** 2 / mean_si ** 2 + 1))\n        dist = sps.lognorm(scale=np.exp(mu_si), s=sigma_si)\n\n        # Discretize the Generation Interval up to 20 days max\n        g_range = np.arange(0, 20)\n        gt = pd.Series(dist.cdf(g_range), index=g_range)\n        gt = gt.diff().fillna(0)\n        gt /= gt.sum()\n        gt = gt.values\n        return gt\n\n    def _get_convolution_ready_gt(self, len_observed):\n        \"\"\" Speeds up theano.scan by pre-computing the generation time interval\n            vector. Thank you to Junpeng Lao for this optimization.\n            Please see the outbreak simulation math here:\n            https://staff.math.su.se/hoehle/blog/2020/04/15/effectiveR0.html \"\"\"\n        gt = self._get_generation_time_interval()\n        convolution_ready_gt = np.zeros((len_observed - 1, len_observed))\n        for t in range(1, len_observed):\n            begin = np.maximum(0, t - len(gt) + 1)\n            slice_update = gt[1 : t - begin + 1][::-1]\n            convolution_ready_gt[\n                t - 1, begin : begin + len(slice_update)\n            ] = slice_update\n        convolution_ready_gt = theano.shared(convolution_ready_gt)\n        return convolution_ready_gt\n\n    def build(self):\n        \"\"\" Builds and returns the Generative model. Also sets self.model \"\"\"\n\n        # p_delay = get_delay_distribution()\n        p_delay = pd.read_csv('https://raw.githubusercontent.com/rtcovidlive/covid-model/master/data/p_delay.csv')\n        nonzero_days = self.observed.total.gt(0)\n        len_observed = len(self.observed)\n        convolution_ready_gt = self._get_convolution_ready_gt(len_observed)\n        x = np.arange(len_observed)[:, None]\n\n        coords = {\n            \"date\": self.observed.index.values,\n            \"nonzero_date\": self.observed.index.values[self.observed.total.gt(0)],\n        }\n        with pm.Model(coords=coords) as self.model:\n\n            # Let log_r_t walk randomly with a fixed prior of ~0.035. Think\n            # of this number as how quickly r_t can react.\n            log_r_t = pm.GaussianRandomWalk(\n                \"log_r_t\",\n                sigma=0.035,\n                dims=[\"date\"]\n            )\n            r_t = pm.Deterministic(\"r_t\", pm.math.exp(log_r_t), dims=[\"date\"])\n\n            # For a given seed population and R_t curve, we calculate the\n            # implied infection curve by simulating an outbreak. While this may\n            # look daunting, it's simply a way to recreate the outbreak\n            # simulation math inside the model:\n            # https://staff.math.su.se/hoehle/blog/2020/04/15/effectiveR0.html\n            seed = pm.Exponential(\"seed\", 1 / 0.02)\n            y0 = tt.zeros(len_observed)\n            y0 = tt.set_subtensor(y0[0], seed)\n            outputs, _ = theano.scan(\n                fn=lambda t, gt, y, r_t: tt.set_subtensor(y[t], tt.sum(r_t * y * gt)),\n                sequences=[tt.arange(1, len_observed), convolution_ready_gt],\n                outputs_info=y0,\n                non_sequences=r_t,\n                n_steps=len_observed - 1,\n            )\n            infections = pm.Deterministic(\"infections\", outputs[-1], dims=[\"date\"])\n\n            # Convolve infections to confirmed positive reports based on a known\n            # p_delay distribution. See patients.py for details on how we calculate\n            # this distribution.\n            test_adjusted_positive = pm.Deterministic(\n                \"test_adjusted_positive\",\n                conv2d(\n                    tt.reshape(infections, (1, len_observed)),\n                    tt.reshape(p_delay, (1, len(p_delay))),\n                    border_mode=\"full\",\n                )[0, :len_observed],\n                dims=[\"date\"]\n            )\n\n            # Picking an exposure with a prior that exposure never goes below\n            # 0.1 * max_tests. The 0.1 only affects early values of Rt when\n            # testing was minimal or when data errors cause underreporting\n            # of tests.\n            tests = pm.Data(\"tests\", self.observed.total.values, dims=[\"date\"])\n            exposure = pm.Deterministic(\n                \"exposure\",\n                pm.math.clip(tests, self.observed.total.max() * 0.1, 1e9),\n                dims=[\"date\"]\n            )\n\n            # Test-volume adjust reported cases based on an assumed exposure\n            # Note: this is similar to the exposure parameter in a Poisson\n            # regression.\n            positive = pm.Deterministic(\n                \"positive\", exposure * test_adjusted_positive,\n                dims=[\"date\"]\n            )\n\n            # Save data as part of trace so we can access in inference_data\n            observed_positive = pm.Data(\"observed_positive\", self.observed.positive.values, dims=[\"date\"])\n            nonzero_observed_positive = pm.Data(\"nonzero_observed_positive\", self.observed.positive[nonzero_days.values].values, dims=[\"nonzero_date\"])\n\n            positive_nonzero = pm.NegativeBinomial(\n                \"nonzero_positive\",\n                mu=positive[nonzero_days.values],\n                alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n                observed=nonzero_observed_positive,\n                dims=[\"nonzero_date\"]\n            )\n\n        return self.model\n\n    def sample(\n        self,\n        cores=4,\n        chains=4,\n        tune=700,\n        draws=200,\n        target_accept=0.95,\n        init=\"jitter+adapt_diag\",\n    ):\n        \"\"\" Runs the PyMC3 model and stores the trace result in self.trace \"\"\"\n\n        if self.model is None:\n            self.build()\n\n        with self.model:\n            self._trace = pm.sample(\n                draws=draws,\n                cores=cores,\n                chains=chains,\n                target_accept=target_accept,\n                tune=tune,\n                init=init,\n            )\n\n        return self\n</code></pre>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/#get-the-data","title":"Get the data","text":"<p>Read the data from sciensano:</p> <pre><code>df_tests = pd.read_csv('https://epistat.sciensano.be/Data/COVID19BE_tests.csv', parse_dates=['DATE'])\n</code></pre> <p>What is in this dataframe ?</p> <pre><code>df_tests\n</code></pre> DATE PROVINCE REGION TESTS_ALL TESTS_ALL_POS 0 2020-03-01 Antwerpen Flanders 18 0 1 2020-03-01 BrabantWallon Wallonia 8 0 2 2020-03-01 Brussels Brussels 4 0 3 2020-03-01 Hainaut Wallonia 5 0 4 2020-03-01 Li\u00e8ge Wallonia 8 0 ... ... ... ... ... ... 2935 2020-10-31 NaN NaN 58 13 2936 2020-10-31 Namur Wallonia 864 387 2937 2020-10-31 OostVlaanderen Flanders 927 106 2938 2020-10-31 VlaamsBrabant Flanders 1600 259 2939 2020-10-31 WestVlaanderen Flanders 512 82 <p>2940 rows \u00d7 5 columns</p> <p>We see that we have the number of tests (TESTS_ALL) and the number of positive tests (TEST_ALL_POS) per date, province and region. In this post, we will analyse the three regions: Brussels, Flanders and Wallonia.</p>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/#preprocessing","title":"Preprocessing","text":"<p>Are the any Nan ?</p> <pre><code>df_tests.isnull().mean()\n</code></pre> <pre><code>DATE             0.000000\nPROVINCE         0.083333\nREGION           0.083333\nTESTS_ALL        0.000000\nTESTS_ALL_POS    0.000000\ndtype: float64\n</code></pre> <p>About eight procent of the lines do not have a PROVINCE nor REGION assigned. What should we do with those ? Ignore them ? Let's look how many there are:</p> <pre><code>ax = df_tests[df_tests['REGION'].isnull()].groupby(['DATE',], dropna=False).sum().plot(figsize=(18, 4))\nax.set(title='Number of covid-19 tests per day not attributed to a region in Belgium', ylabel='Number of tests');\n</code></pre> <p></p> <pre><code>#hide \n# (df_tests\n#     .fillna('Nan')\n#     .groupby(['DATE','REGION'], as_index=False)['TESTS_ALL']\n#     .sum().set_index('REGION')['TESTS_ALL']\n# #    .assign(total=lambda d:d[['Brussels',    'Flanders', 'Wallonia']].sum(axis=1))\n# )\n</code></pre> <pre><code>REGION\nBrussels        4\nFlanders       56\nNan             1\nWallonia       21\nBrussels       17\n            ...  \nWallonia    21127\nBrussels     1237\nFlanders     3515\nNan            58\nWallonia     2806\nName: TESTS_ALL, Length: 980, dtype: int64\n</code></pre> <p>Here we create a function that distributes the non attributed tests according to the number of tests in each region. For example suppose on a day there are 10, 20 and 150 test in Brussels, Flanders and Wallonia respectively. Suppose there are 10 test unattributed in Flanders. Then we add 10 * (10/(10+20+150)) = 10 * (10/180) = 100/180 = 0.55 test to Flanders. The total number of test of Flanders becomes 10.55. We round that to the nearest integer: 11. And we do the same for the other regions Brussels and Wallonia. So we distribute the 10 unattributed tests weighted according to the number of tests in each region.</p> <pre><code>def redistribute(g, col):\n  gdata = g.groupby('REGION')[col].sum()\n  gdata.loc['Brussels'] += gdata.loc['Nan'] * (gdata.loc['Brussels']/(gdata.loc['Brussels'] + gdata.loc['Flanders'] + gdata.loc['Wallonia']))\n  gdata.loc['Flanders'] += gdata.loc['Nan'] * (gdata.loc['Flanders']/(gdata.loc['Brussels'] + gdata.loc['Flanders'] + gdata.loc['Wallonia']))\n  gdata.loc['Wallonia'] += gdata.loc['Nan'] * (gdata.loc['Wallonia']/(gdata.loc['Brussels'] + gdata.loc['Flanders'] + gdata.loc['Wallonia']))\n  gdata.drop(index='Nan', inplace=True)\n  gdata = np.round(gdata.fillna(0)).astype(int)\n  return gdata\n</code></pre> <pre><code># Redistribute the nan for the column TESTS_ALL\ndf_tests_all = (df_tests\n    .fillna('Nan')\n    .groupby(['DATE'])\n    .apply(redistribute, 'TESTS_ALL')\n    .stack()\n    .reset_index()\n    .rename(columns={'DATE':'date', 'REGION':'region', 0:'total'})\n)\n</code></pre> <pre><code># Redistribute the nan for the column TESTS_ALL_POS\ndf_tests_positive = (df_tests\n    .fillna('Nan')\n    .groupby(['DATE'])\n    .apply(redistribute, 'TESTS_ALL_POS')\n    .stack()\n    .reset_index()\n    .rename(columns={'DATE':'date', 'REGION':'region', 0:'positive'})\n)\n</code></pre> <pre><code>#hide\n\n# (df_tests\n#     .fillna('Nan')\n#     .groupby(['DATE','REGION'])['TESTS_ALL']\n#     .sum().to_frame().unstack()\n#  #   .rename(pandas.index.get_level_values(0), axis='columns')\n#     .assign(total=lambda d:d.drop(columns=('TESTS_ALL', 'Nan')).sum(axis=1))\n# )\n</code></pre> <pre><code># Combine the total number of tests and the number of positive tests into a basetable\ndf_tests_per_region_day = pd.concat([df_tests_all, df_tests_positive['positive']], axis=1).set_index(['region', 'date'])\n</code></pre> <pre><code># Check if the basetable is ok\nassert df_tests_per_region_day.isnull().sum().sum() == 0, 'There are nan in the basetable'\n</code></pre> <pre><code>#hide\n\n# Reformat data into Rtlive format\n# df_tests_per_region_day = (df_tests\n  #  .groupby(['REGION', 'DATE'], as_index=False)\n  #  .agg(positive=('TESTS_ALL_POS', 'sum'), total=('TESTS_ALL', 'sum'))\n  #  .rename(columns={'REGION':'region', 'DATE':'date'})\n  #  .set_index([\"region\", \"date\"])\n  #  )\n</code></pre> <pre><code>df_tests_per_region_day\n</code></pre> total positive region date Brussels 2020-03-01 4 0 Flanders 2020-03-01 57 0 Wallonia 2020-03-01 21 0 Brussels 2020-03-02 17 3 Flanders 2020-03-02 259 7 ... ... ... 2020-10-30 38154 6531 Wallonia 2020-10-30 21414 9011 Brussels 2020-10-31 1246 392 Flanders 2020-10-31 3542 531 Wallonia 2020-10-31 2827 951 <p>735 rows \u00d7 2 columns</p> <pre><code># What regions do we have in the table ?\ndf_tests_per_region_day.index.get_level_values(0).unique().to_list()\n</code></pre> <pre><code>['Brussels', 'Flanders', 'Wallonia']\n</code></pre>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/#ret-for-flanders","title":"Re(t) for Flanders","text":"<pre><code>region = 'Flanders'\n</code></pre> <pre><code>ax = df_tests_per_region_day.loc[region].plot(figsize=(18,6))\nax.set(title=f'Number of tests for covid-19 and number of positives in {region}');\n</code></pre> <pre><code>import datetime\nfrom dateutil.relativedelta import relativedelta\n</code></pre> <pre><code># Remove last two days because tests are not yet fully reported\ntoday_minus_two = datetime.datetime.today() + relativedelta(days=-2)\ntoday_minus_two.strftime(\"%Y-%m-%d\")\n</code></pre> <pre><code>'2020-10-30'\n</code></pre> <pre><code>ax = df_tests_per_region_day.loc[region][:today_minus_two].plot(figsize=(18,6))\nax.set(title=f'Number of tests for covid-19 and number of positives in {region}');\n</code></pre> <pre><code># Fit the model on the data\ndf = df_tests_per_region_day.loc[region][:today_minus_two]\ngm = GenerativeModel(region, df)\ngm.sample()\n</code></pre> <pre><code>Only 200 samples in chain.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, seed, log_r_t]\n</code></pre>    100.00% [3600/3600 1:10:58&lt;00:00 Sampling 4 chains, 0 divergences]  <pre><code>Sampling 4 chains for 700 tune and 200 draw iterations (2_800 + 800 draws total) took 4260 seconds.\n\n\n\n\n\n&lt;__main__.GenerativeModel at 0x7f382109add8&gt;\n</code></pre> <pre><code>#collapse\n\ndef summarize_inference_data(inference_data: az.InferenceData):\n    \"\"\" Summarizes an inference_data object into the form that we publish on rt.live \"\"\"\n    posterior = inference_data.posterior\n    hdi_mass = 80\n    hpdi = az.hdi(posterior.r_t, hdi_prob=hdi_mass / 100).r_t\n\n    observed_positive = inference_data.constant_data.observed_positive.to_series()\n    scale_to_positives = lambda data: observed_positive.mean() / np.mean(data) * data\n    tests = inference_data.constant_data.tests.to_series()\n    normalized_positive = observed_positive / tests.clip(0.1 * tests.max())\n\n    summary = pd.DataFrame(\n        data={\n            \"mean\": posterior.r_t.mean([\"draw\", \"chain\"]),\n            \"median\": posterior.r_t.median([\"chain\", \"draw\"]),\n            f\"lower_{hdi_mass}\": hpdi[:, 0],\n            f\"upper_{hdi_mass}\": hpdi[:, 1],\n            \"infections\": scale_to_positives(\n                posterior.infections.mean([\"draw\", \"chain\"])\n            ),\n            \"test_adjusted_positive\": scale_to_positives(\n                posterior.test_adjusted_positive.mean([\"draw\", \"chain\"])\n            ),\n            \"test_adjusted_positive_raw\": scale_to_positives(normalized_positive),\n            \"positive\": observed_positive,\n            \"tests\": tests,\n        },\n        index=pd.Index(posterior.date.values, name=\"date\"),\n    )\n    return summary\n</code></pre> <pre><code>result = summarize_inference_data(gm.inference_data)\n</code></pre>    100.00% [800/800 00:12&lt;00:00]  <pre><code>fig, ax = plt.subplots(figsize=(12, 8))\nresult.infections.plot(c=\"C2\", label=\"Expected primary infections\")\nresult.test_adjusted_positive.plot(c=\"C0\", label=\"Expected positive tests if tests were constant\")\nresult.test_adjusted_positive_raw.plot(c=\"C1\", alpha=.5, label=\"Expected positive tests\", style=\"--\")\ngm.observed.positive.plot(c=\"C7\", alpha=.7, label=\"Reported positive tests\")\nfig.set_facecolor(\"w\")\nax.legend();\nax.set(title=f\"rt.live model inference for {region}\", ylabel=\"number of cases\")\nsns.despine();\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(12, 8))\n\nax.set(title=f\"Effective reproduction number for {region}\", ylabel=\"$R_e(t)$\")\nsamples = gm.trace[\"r_t\"]\nx = result.index\ncmap = plt.get_cmap(\"Reds\")\npercs = np.linspace(51, 99, 40)\ncolors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs))\nsamples = samples.T\n\nresult[\"median\"].plot(c=\"k\", ls='-')\n\nfor i, p in enumerate(percs[::-1]):\n    upper = np.percentile(samples, p, axis=1)\n    lower = np.percentile(samples, 100-p, axis=1)\n    color_val = colors[i]\n    ax.fill_between(x, upper, lower, color=cmap(color_val), alpha=.8)\n\nax.axhline(1.0, c=\"k\", lw=1, linestyle=\"--\")\nsns.despine();\n</code></pre>"},{"location":"2020/11/01/estimating-the-effective-reproduction-number-in-belgium/#ret-for-wallonia","title":"Re(t) for Wallonia","text":"<pre><code>region = 'Wallonia'\n</code></pre> <pre><code>ax = df_tests_per_region_day.loc[region].plot(figsize=(18,6))\nax.set(title=f'Number of tests for covid-19 and number of positives in {region}');\n</code></pre> <pre><code># Remove last two days because tests are not yet fully reported\ntoday_minus_two = datetime.datetime.today() + relativedelta(days=-2)\ntoday_minus_two.strftime(\"%Y-%m-%d\")\n</code></pre> <pre><code>'2020-10-30'\n</code></pre> <pre><code>ax = df_tests_per_region_day.loc[region][:today_minus_two].plot(figsize=(18,6))\nax.set(title=f'Number of tests for covid-19 and number of positives in {region}');\n</code></pre> <pre><code>df = df_tests_per_region_day.loc[region][:today_minus_two]\ngm = GenerativeModel(region, df)\ngm.sample()\n</code></pre> <pre><code>Only 200 samples in chain.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, seed, log_r_t]\n</code></pre>    100.00% [3600/3600 1:12:49&lt;00:00 Sampling 4 chains, 0 divergences]  <pre><code>Sampling 4 chains for 700 tune and 200 draw iterations (2_800 + 800 draws total) took 4371 seconds.\n\n\n\n\n\n&lt;__main__.GenerativeModel at 0x7f380ac35c88&gt;\n</code></pre> <pre><code>result = summarize_inference_data(gm.inference_data)\n</code></pre>    100.00% [800/800 00:12&lt;00:00]  <pre><code>fig, ax = plt.subplots(figsize=(12, 8))\nresult.infections.plot(c=\"C2\", label=\"Expected primary infections\")\nresult.test_adjusted_positive.plot(c=\"C0\", label=\"Expected positive tests if tests were constant\")\nresult.test_adjusted_positive_raw.plot(c=\"C1\", alpha=.5, label=\"Expected positive tests\", style=\"--\")\ngm.observed.positive.plot(c=\"C7\", alpha=.7, label=\"Reported positive tests\")\nfig.set_facecolor(\"w\")\nax.legend();\nax.set(title=f\"rt.live model inference for {region}\", ylabel=\"number of cases\")\nsns.despine();\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(12, 8))\n\nax.set(title=f\"Effective reproduction number for {region}\", ylabel=\"$R_e(t)$\")\nsamples = gm.trace[\"r_t\"]\nx = result.index\ncmap = plt.get_cmap(\"Reds\")\npercs = np.linspace(51, 99, 40)\ncolors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs))\nsamples = samples.T\n\nresult[\"median\"].plot(c=\"k\", ls='-')\n\nfor i, p in enumerate(percs[::-1]):\n    upper = np.percentile(samples, p, axis=1)\n    lower = np.percentile(samples, 100-p, axis=1)\n    color_val = colors[i]\n    ax.fill_between(x, upper, lower, color=cmap(color_val), alpha=.8)\n\nax.axhline(1.0, c=\"k\", lw=1, linestyle=\"--\")\nsns.despine();\n</code></pre> <pre><code>\n</code></pre>"},{"location":"2020/11/05/estimating-the-effective-reproduction-number-in-belgium-with-the-rki-method/","title":"Estimating the effective reproduction number in Belgium with the RKI method","text":"<p>Using the Robert Koch Institute method with serial interval of 4.</p> <p>Every day Bart Mesuere tweets a nice dashboard with current numbers about Covid-19 in Belgium. This was the tweet on Wednesday 20/11/04:</p> <p>twitter: https://twitter.com/BartMesuere/status/1323881489864548352</p> <p>It's nice to see that the effective reproduction number ($Re(t)$) is again below one. That means the power of virus is declining and the number of infection will start to lower. This occured first on Tuesday 2020/11/3:</p> <p>twitter: https://twitter.com/BartMesuere/status/1323519613855059968</p> <p>I estimated the $Re(t)$ earlier with rt.live model in this notebook. There the $Re(t)$ was still estimated to be above one. Michael Osthege replied with a simulation results with furter improved model:</p> <p>twitter: https://twitter.com/theCake/status/1323211910481874944</p> <p>In that estimation, the $Re(t)$ was also not yet heading below one at the end of october.</p> <p>In this notebook, we will implement a calculation based on the method of the Robert Koch Institute. The method is described and programmed in R in this blog post.</p> <p>In that blogpost there's a link to a website with estimations for most places in the world The estimation for Belgium is here</p> <p></p> <p>According to that calculation, $Re(t)$ is already below zero for some days.</p>"},{"location":"2020/11/05/estimating-the-effective-reproduction-number-in-belgium-with-the-rki-method/#load-libraries-and-data","title":"Load libraries and data","text":"<pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <pre><code>df_tests = pd.read_csv('https://epistat.sciensano.be/Data/COVID19BE_tests.csv', parse_dates=['DATE'])\n</code></pre> <pre><code>df_cases = pd.read_csv('https://epistat.sciensano.be/Data/COVID19BE_CASES_AGESEX.csv', parse_dates=['DATE'])\ndf_cases\n</code></pre> DATE PROVINCE REGION AGEGROUP SEX CASES 0 2020-03-01 Antwerpen Flanders 40-49 M 1 1 2020-03-01 Brussels Brussels 10-19 F 1 2 2020-03-01 Brussels Brussels 10-19 M 1 3 2020-03-01 Brussels Brussels 20-29 M 1 4 2020-03-01 Brussels Brussels 30-39 F 1 ... ... ... ... ... ... ... 36279 NaT VlaamsBrabant Flanders 40-49 M 3 36280 NaT VlaamsBrabant Flanders 50-59 M 1 36281 NaT WestVlaanderen Flanders 20-29 F 1 36282 NaT WestVlaanderen Flanders 50-59 M 3 36283 NaT NaN NaN NaN NaN 1 <p>36284 rows \u00d7 6 columns</p> <p>Reformat data into Rtlive format</p> <pre><code>df_cases_per_day = (df_cases\n   .dropna(subset=['DATE'])\n   .assign(region='Belgium')\n   .groupby(['region', 'DATE'], as_index=False)\n   .agg(cases=('CASES', 'sum'))\n   .rename(columns={'DATE':'date'})\n   .set_index([\"region\", \"date\"])\n)\n</code></pre> <p>What's in our basetable:</p> <pre><code>df_cases_per_day\n</code></pre> cases region date Belgium 2020-03-01 19 2020-03-02 19 2020-03-03 34 2020-03-04 53 2020-03-05 81 ... ... 2020-11-01 2660 2020-11-02 13345 2020-11-03 11167 2020-11-04 4019 2020-11-05 5 <p>250 rows \u00d7 1 columns</p> <p>Let's plot the number of cases in function of the time.</p> <pre><code>ax = df_cases_per_day.loc['Belgium'].plot(figsize=(18,6))\nax.set(ylabel='Number of cases', title='Number of cases for covid-19 and number of positives in Belgium');\n</code></pre> <p></p> <p>We see that the last days are not yet complete. Let's cut off the last two days of reporting.</p> <pre><code>import datetime\nfrom dateutil.relativedelta import relativedelta\n</code></pre> <p>Calculate the date two days ago:</p> <pre><code>datetime.date(2020, 11, 3)\n</code></pre> <pre><code>datetime.date(2020, 11, 3)\n</code></pre> <pre><code># today_minus_two = datetime.date.today() + relativedelta(days=-2)\ntoday_minus_two = datetime.date(2020, 11, 3) # Fix the day\ntoday_minus_two.strftime(\"%Y-%m-%d\")\n</code></pre> <pre><code>'2020-11-03'\n</code></pre> <p>Replot the cases:</p> <pre><code>ax = df_cases_per_day.loc['Belgium'][:today_minus_two].plot(figsize=(18,6))\nax.set(ylabel='Number of cases', title='Number of cases for covid-19 and number of positives in Belgium');\n</code></pre> <p></p> <p>Select the Belgium region:</p> <pre><code>region = 'Belgium'\ndf = df_cases_per_day.loc[region][:today_minus_two]\ndf\n</code></pre> cases date 2020-03-01 19 2020-03-02 19 2020-03-03 34 2020-03-04 53 2020-03-05 81 ... ... 2020-10-30 15185 2020-10-31 6243 2020-11-01 2660 2020-11-02 13345 2020-11-03 11167 <p>248 rows \u00d7 1 columns</p> <p>Check the types of the columns:</p> <pre><code>df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 248 entries, 2020-03-01 to 2020-11-03\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   cases   248 non-null    int64\ndtypes: int64(1)\nmemory usage: 3.9 KB\n</code></pre>"},{"location":"2020/11/05/estimating-the-effective-reproduction-number-in-belgium-with-the-rki-method/#robert-koch-institute-method","title":"Robert Koch Institute method","text":"<p>A basic method to calculate the effective reproduction number is described (among others) in this blogpost. I included the relevant paragraph:</p> <p>In a recent report (an der Heiden and Hamouda 2020) the RKI described their method for computing R as part of the COVID-19 outbreak as follows (p. 13): For a constant generation time of 4 days, one obtains R  as the ratio of new infections in two consecutive time periods each consisting of 4 days. Mathematically, this estimation could be formulated as part of a statistical model:</p> <p>$$y_{s+4} | y_{s} \\sim Po(R \\cdot y_{s}), s= 1,2,3,4$$</p> <p>where $y_{1}, \\ldots, y_{4}$ are considered as fixed. From this we obtain </p> <p>$$\\hat{R}{RKI} = \\sum$$}^{4} y_{s+4} / \\sum_{s=1}^{4} y_{s</p> <p>Somewhat arbitrary, we denote by $Re(t)$ the above estimate for  R when $s=1$ corresponds to time $t-8$, i.e. we assign the obtained value to the last of the 8 values used in the computation.</p> <p>In Python, we define a lambda function that we apply on a rolling window. Since indexes start from zero, we calculate:</p> <p>$$\\hat{R}{RKI} = \\sum$$}^{3} y_{s+4} / \\sum_{s=0}^{3} y_{s</p> <pre><code>rt = lambda y: np.sum(y[4:])/np.sum(y[:4])\n</code></pre> <pre><code>df.rolling(8).apply(rt)\n</code></pre> cases date 2020-03-01 NaN 2020-03-02 NaN 2020-03-03 NaN 2020-03-04 NaN 2020-03-05 NaN ... ... 2020-10-30 1.273703 2020-10-31 0.929291 2020-11-01 0.601838 2020-11-02 0.499806 2020-11-03 0.475685 <p>248 rows \u00d7 1 columns</p> <p>The first values are Nan because the window is in the past. If we plot the result, it looks like this:</p> <pre><code>ax = df.rolling(8).apply(rt).plot(figsize=(16,4), label='Re(t)')\nax.set(ylabel='Re(t)', title='Effective reproduction number estimated with RKI method')\nax.legend(['Re(t)']);\n</code></pre> <p></p> <p>To avoid the spikes due to weekend reporting issue, I first applied a rolling mean on a window of 7 days:</p> <pre><code>ax = df.rolling(7).mean().rolling(8).apply(rt).plot(figsize=(16,4), label='Re(t)')\nax.set(ylabel='Re(t)', title='Effective reproduction number estimated with RKI method after rolling mean on window of 7 days')\nax.legend(['Re(t)']);\n</code></pre> <p></p>"},{"location":"2020/11/05/estimating-the-effective-reproduction-number-in-belgium-with-the-rki-method/#interactive-visualisation-in-altair","title":"Interactive visualisation in Altair","text":"<pre><code>import altair as alt\n\nalt.Chart(df.rolling(7).mean().rolling(8).apply(rt).fillna(0).reset_index()).mark_line().encode(\n    x=alt.X('date:T'),\n    y=alt.Y('cases', title='Re(t)'),\n    tooltip=['date:T', alt.Tooltip('cases', format='.2f')]\n).transform_filter(\n    alt.datum.date &gt; alt.expr.toDate('2020-03-13')\n).properties(\n    width=600,\n    title='Effective reproduction number in Belgium based on Robert-Koch Institute method'\n)\n</code></pre>"},{"location":"2020/11/05/estimating-the-effective-reproduction-number-in-belgium-with-the-rki-method/#making-the-final-visualisation-in-altair","title":"Making the final visualisation in Altair","text":"<p>In the interactive Altair figure below, we show the $Re(t)$ for the last 14 days. We reduce the rolling mean window to three to see faster reactions.</p> <pre><code>#collapse\n\ndf_plot = df.rolling(7).mean().rolling(8).apply(rt).fillna(0).reset_index()\nlast_value = str(df_plot.iloc[-1]['cases'].round(2)) + ' \u2193'\nfirst_value = str(df_plot[df_plot['date'] == '2020-10-21'].iloc[0]['cases'].round(2)) # + ' \u2191'\ntoday_minus_15 = datetime.datetime.today() + relativedelta(days=-15)\ntoday_minus_15_str = today_minus_15.strftime(\"%Y-%m-%d\")\n\nline = alt.Chart(df_plot).mark_line(point=True).encode(\n    x=alt.X('date:T', axis=alt.Axis(title='Datum', grid=False)),\n    y=alt.Y('cases', axis=alt.Axis(title='Re(t)', grid=False, labels=False, titlePadding=40)),\n    tooltip=['date:T', alt.Tooltip('cases', title='Re(t)', format='.2f')]\n).transform_filter(\n    alt.datum.date &gt; alt.expr.toDate(today_minus_15_str)\n).properties(\n    width=600,\n    height=100\n)\n\nhline = alt.Chart(pd.DataFrame({'cases': [1]})).mark_rule().encode(y='cases')\n\n\nlabel_right = alt.Chart(df_plot).mark_text(\n    align='left', dx=5, dy=-10 , size=15\n).encode(\n    x=alt.X('max(date):T', title=None),\n    text=alt.value(last_value),\n)\n\nlabel_left = alt.Chart(df_plot).mark_text(\n    align='right', dx=-5, dy=-40, size=15\n).encode(\n    x=alt.X('min(date):T', title=None),\n    text=alt.value(first_value),\n).transform_filter(\n    alt.datum.date &gt; alt.expr.toDate(today_minus_15_str)\n)\n\nsource = alt.Chart(\n    {\"values\": [{\"text\": \"Data source: Sciensano\"}]}\n).mark_text(size=12, align='left', dx=-57).encode(\n    text=\"text:N\"\n)\n\nalt.vconcat(line + label_left + label_right + hline, source).configure(\n    background='#D9E9F0'\n).configure_view(\n    stroke=None, # Remove box around graph\n).configure_axisY(\n    ticks=False,\n    grid=False,\n    domain=False\n).configure_axisX(\n    grid=False,\n    domain=False\n).properties(title={\n      \"text\": ['Effective reproduction number for the last 14 days in Belgium'], \n      \"subtitle\": [f'Estimation based on the number of cases until {today_minus_two.strftime(\"%Y-%m-%d\")} after example of Robert Koch Institute with serial interval of 4'],\n}\n)\n# .configure_axisY(\n#     labelPadding=50,\n# )\n</code></pre> <p>To check the calculation, here are the last for values for the number of cases after applying the mean window of 7:</p> <pre><code>df.rolling(7).mean().iloc[-8:-4]\n</code></pre> cases date 2020-10-27 16067.571429 2020-10-28 16135.857143 2020-10-29 15744.571429 2020-10-30 15218.000000 <p>Those must be added together:</p> <pre><code>df.rolling(7).mean().iloc[-8:-4].sum()\n</code></pre> <pre><code>cases    63166.0\ndtype: float64\n</code></pre> <p>And here are the four values, starting four days ago:</p> <pre><code>df.rolling(7).mean().iloc[-4:]\n</code></pre> cases date 2020-10-31 14459.428571 2020-11-01 14140.428571 2020-11-02 13213.428571 2020-11-03 11641.428571 <p>These are added together:</p> <pre><code>df.rolling(7).mean().iloc[-4:].sum()\n</code></pre> <pre><code>cases    53454.714286\ndtype: float64\n</code></pre> <p>And now we divide those two sums to get the $Re(t)$ of 2020-11-03:</p> <pre><code>df.rolling(7).mean().iloc[-4:].sum()/df.rolling(7).mean().iloc[-8:-4].sum()\n</code></pre> <pre><code>cases    0.846258\ndtype: float64\n</code></pre> <p>This matches (as expected) the value in the graph. Let's compare with three other sources:</p> <ol> <li>Alas it does not match the calculation reported by Bart Mesuere on 2020-11-03 based on the RKI model that reports 0.96:</li> </ol> <p>twitter: https://twitter.com/BartMesuere/status/1323519613855059968</p> <ol> <li> <p>Also, the more elaborated model from rtliveglobal is not yet that optimistic. Mind that model rtlive start estimating the $Re(t)$ from the number of tests instead of the number of cases. It might be that other reporting delays are involved.</p> </li> <li> <p>epiforecast.io is already below 1 since beginning of November.</p> </li> </ol> <p>Another possiblity is that I made somewhere a mistake. If you spot it, please let me know.</p> <pre><code>\n</code></pre>"},{"location":"2022/04/18/reconstructing-economist-graph-with-altair/","title":"\"Reconstructing Economist graph with Altair\"","text":"<p>\"#30DayChartChallenge #altair #day12\"</p> <ul> <li>image: images/Economist_stye%3B_30dayschartchallenge_day12.png</li> </ul> <p>In an Economist article \"The metamorphosis: How Jeremy Corbyn took control of Labour\", the following graph appeared:</p> <p></p> <p>Later, Sarah Leo, data visualiser at The Economist, improved the graph to:</p> <p> The rationale behind this improvement is discussed in her article: 'Mistakes, we made a few'.</p> <p>In this article, I show how visualisation library Altair can be used to reconstruct the improved graph.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport altair as alt\n</code></pre> <p>Read the data for the graph into a Pandas dataframe:</p> <pre><code>df = pd.read_csv('http://infographics.economist.com/databank/Economist_corbyn.csv').dropna()\n</code></pre> <p>This is how the data looks:</p> <pre><code>df\n</code></pre> Page Average number of likes per Facebook post 2016 0 Jeremy Corbyn 5210.0 1 Labour Party 845.0 2 Momentum 229.0 3 Owen Smith 127.0 4 Andy Burnham 105.0 5 Saving Labour 56.0 <p>A standard bar graph in Altair gives this:</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x='Average number of likes per Facebook post 2016:Q',\n    y='Page:O'\n)\n</code></pre> <p>The message of the graph is that Jerermy Corbyn has by far the most likes per Facebook post in 2016. There are a number of improvements possible:</p> <p>The number on the x-axis are multiple of thousands. In spirit of removing as much inkt as possible, let's rescale the x-asis with factor 1000. The label 'Page' on the y-axis is superfluous. Let's remove it.</p> <pre><code>df['page1k'] = df['Average number of likes per Facebook post 2016']/1000.0\n</code></pre> <p>After scaling the graphs looks like this:</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k', title='Average number of likes per Facebook post 2016'),\n    y=alt.Y('Page:O', title='')\n)\n</code></pre> <p>A third improvement is to sort the bars from high to low. This supports the message, Jeremy Corbyn has the most clicks.</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k:Q', title='Average number of likes per Facebook post 2016'),\n    y=alt.Y('Page:O', title='', sort=alt.EncodingSortField(\n            field=\"Average number of likes per Facebook post 2016:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ))\n)\n</code></pre> <p>Now, we see that we have to many ticks on the x-axis. We can add a scale and map the x-axis to integers to cope with that. While adding markup for the x-axis, we add orient='top'. That move the xlabel text to the top of the graph.</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k:Q', title='Average number of likes per Facebook post 2016',\n            axis=alt.Axis(title='Average number of likes per Facebook post 2016', orient=\"top\", format='d', values=[1,2,3,4,5,6]),\n            scale=alt.Scale(round=True, domain=[0,6])),\n    y=alt.Y('Page:O', title='', sort=alt.EncodingSortField(\n            field=\"Average number of likes per Facebook post 2016:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ))\n)\n</code></pre> <p>Now, we want to remove the x-axis itself as it adds nothing extra. We do that by putting the stroke at None in the configure_view. We also adjust the x-axis title to make clear the numbers are multiples of thousands.</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k:Q', title=\"Average number of likes per Facebook post 2016  ('000)\",\n            axis=alt.Axis(title='Average number of likes per Facebook post 2016', orient=\"top\", format='d', values=[1,2,3,4,5,6]),\n            scale=alt.Scale(round=True, domain=[0,6])),\n    y=alt.Y('Page:O', title='', sort=alt.EncodingSortField(\n            field=\"Average number of likes per Facebook post 2016:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ))\n).configure_view(\n    stroke=None, # Remove box around graph\n)\n</code></pre> <p>Next we try to left align the y-axis labels:</p> <pre><code>alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k:Q',\n            axis=alt.Axis(title=\"Average number of likes per Facebook post 2016  ('000)\", orient=\"top\", format='d', values=[1,2,3,4,5,6]),\n            scale=alt.Scale(round=True, domain=[0,6])),\n    y=alt.Y('Page:O', title='', sort=alt.EncodingSortField(\n            field=\"Average number of likes per Facebook post 2016:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ))\n).configure_view(\n    stroke=None, # Remove box around graph\n).configure_axisY(\n    labelPadding=70, \n    labelAlign='left'\n)\n</code></pre> <p>Now, we apply the Economist style:</p> <pre><code>square = alt.Chart().mark_rect(width=50, height=18, color='#EB111A', xOffset=-105, yOffset=10)\n\nbars = alt.Chart(df).mark_bar().encode(\n    x=alt.X('page1k:Q',\n            axis=alt.Axis(title=\"\", orient=\"top\", format='d', values=[1,2,3,4,5,6], labelFontSize=14),\n            scale=alt.Scale(round=True, domain=[0,6])),\n    y=alt.Y('Page:O', title='', sort=alt.EncodingSortField(\n            field=\"Average number of likes per Facebook post 2016:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ),\n        # Based on https://stackoverflow.com/questions/66684882/color-some-x-labels-in-altair-plot\n        axis=alt.Axis(labelFontSize=14, labelFontStyle=alt.condition('datum.value == \"Jeremy Corbyn\"', alt.value('bold'), alt.value('italic'))))\n).properties(title={\n      \"text\": [\"Left Click\", ], \n      \"subtitle\": [\"Average number of likes per Facebook post\\n\", \"2016, '000\"],\n      \"align\": 'left',\n      \"anchor\": 'start'\n    }\n)\n\nsource = alt.Chart(\n    {\"values\": [{\"text\": \"Source: Facebook\"}]}\n).mark_text(size=12, align='left', dx=-120, color='darkgrey').encode(\n    text=\"text:N\"\n)\n\n# from https://stackoverflow.com/questions/57244390/has-anyone-figured-out-a-workaround-to-add-a-subtitle-to-an-altair-generated-cha\nchart = alt.vconcat(\n    square,\n    bars,\n    source\n).configure_concat(\n    spacing=0\n).configure(\n    background='#D9E9F0'\n).configure_view(\n    stroke=None, # Remove box around graph\n).configure_axisY(\n    labelPadding=110,\n    labelAlign='left',\n    ticks=False,\n    grid=False\n).configure_title(\n    fontSize=22,\n    subtitleFontSize=18,\n    offset=30,\n    dy=30\n)\n\nchart\n</code></pre> <p>The only thing, I could not reproduce with Altair is the light bar around the the first label and bar. For those final touches I think it's better to export the graph and add those finishing touches with a tool such as Inkscape or Illustrator.</p>"},{"location":"2022/04/24/bar-chart-made-in-altair-with-financial-times-style/","title":"\"Bar chart made in Altair with Financial Times style\"","text":"<p>\"#30DayChartChallenge #Day24 Themeday: Financial times\"</p> <ul> <li>image: images/barchart_FT_style_altair.png</li> </ul> <pre><code>import pandas as pd\nimport altair as alt\n</code></pre> <p>The #30DayChartChallenge Day 24 calls for Financial Times themed charts. The bar chart that I will try to reproduce in Altair was published in the article: \"Financial warfare: will there be a backlash against the dollar?\"</p> <p>This is the graph (without FT background) to we want to reproduce: </p> <p>I digitized the heights of yhe bars with WebplotDigitizer:</p> <pre><code>data = \"\"\"Bar0, 3.23\nBar1, 1.27\nBar2, 1.02\nBar3, 0.570\nBar4, 0.553\nBar5, 0.497\nBar6, 0.467\nBar7, 0.440\nBar8, 0.420\nBar9, 0.413\nBar10, 0.317\nBar11, 0.0433\"\"\"\n\ndata_values = [float(x.split()[1]) for x in data.splitlines()]\n</code></pre> <p>I put the values into a Pandas dataframe:</p> <pre><code>source = pd.DataFrame({\n    'label': ['China', 'Japan', 'Switserland', 'India', 'Taiwan', 'Hong Kong', 'Russia', 'South Korea', 'Saudi Arabia', 'Singapore', 'Eurozone', 'US'],\n    'val': data_values\n})\n</code></pre> <p>Now we build the graph and alter it's style to resemble the Financial Times style:</p> <pre><code>square = alt.Chart().mark_rect(width=80, height=5, color='black', xOffset=-112, yOffset=10)\n\nbars = alt.Chart(source).mark_bar(color='#174C7F', size=30).encode(\n    x=alt.X('val:Q', title='', axis=alt.Axis(tickCount=6, domain=False, labelColor='darkgray'), scale=alt.Scale(domain=[0, 3.0])),\n    y=alt.Y('label:N', title='', sort=alt.EncodingSortField(\n            field=\"val:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ), axis=alt.Axis(domainColor='lightgray',\n                         labelFontSize=18, labelColor='darkgray', labelPadding=5,\n                         labelFontStyle='Bold',\n                         tickSize=18, tickColor='lightgray'))\n).properties(title={\n      \"text\": [\"The biggest holders of FX reserves\", ], \n      \"subtitle\": [\"Official foreign exchange reserve (Jan 2022, $tn)\"],\n      \"align\": 'left',\n      \"anchor\": 'start'\n    },\n    width=700,\n    height=512\n)\n\nsource_text = alt.Chart(\n    {\"values\": [{\"text\": \"Source: IMF, \u00a9 FT\"}]}\n).mark_text(size=12, align='left', dx=-140, color='darkgrey').encode(\n    text=\"text:N\"\n)\n\n# from https://stackoverflow.com/questions/57244390/has-anyone-figured-out-a-workaround-to-add-a-subtitle-to-an-altair-generated-cha\nchart = alt.vconcat(\n    square,\n    bars,\n    source_text\n).configure_concat(\n    spacing=0\n).configure(\n    background='#fff1e5',\n).configure_view(\n    stroke=None, # Remove box around graph\n).configure_title(\n    # font='metricweb',\n    fontSize=22,\n    fontWeight=400,\n    subtitleFontSize=18,\n    subtitleColor='darkgray',\n    subtitleFontWeight=400,\n    subtitlePadding=15,\n    offset=80,\n    dy=40\n)\n\nchart\n</code></pre>"},{"location":"2022/04/24/bar-chart-made-in-altair-with-financial-times-style/#trying-to-use-the-offical-financial-times-fonts","title":"Trying to use the offical Financial Times fonts","text":"<p>The chart looks quit similar to the original. Biggest difference is the typography. The Financial times uses its own Metric Web and Financier Display Web fonts and Altair can only use fonts available in the browser. </p> <p>The fonts could be made available via CSS:</p> <pre><code>@font-face {\n    font-family: 'metricweb';\n    src: url('https://www.ft.com/__origami/service/build/v2/files/o-fonts-assets@1.5.0/MetricWeb-Regular.woff2''\n);\n}\n</code></pre> <pre><code>from IPython.display import HTML\nfrom google.colab.output import _publish as publish\npublish.css(\"\"\"@font-face {\n    font-family: 'metricweb', sans-serif;\n    src: url('https://www.ft.com/__origami/service/build/v2/files/o-fonts-assets@1.5.0/MetricWeb-Regular.woff2') format('woff2');\n}\"\"\")\n</code></pre> <pre><code>square = alt.Chart().mark_rect(width=80, height=5, color='black', xOffset=-112, yOffset=10)\n\nbars = alt.Chart(source).mark_bar(color='#174C7F', size=30).encode(\n    x=alt.X('val:Q', title='', axis=alt.Axis(tickCount=6, domain=False), scale=alt.Scale(domain=[0, 3.0])),\n    y=alt.Y('label:N', title='', sort=alt.EncodingSortField(\n            field=\"val:Q\",  # The field to use for the sort\n            op=\"sum\",  # The operation to run on the field prior to sorting\n            order=\"ascending\"  # The order to sort in\n        ), axis=alt.Axis(domainColor='lightgray',\n                         labelFontSize=18, labelColor='darkgray', labelPadding=5,\n                         labelFontStyle='Bold',\n                         tickSize=18, tickColor='lightgray'))\n).properties(title={\n      \"text\": [\"The biggest holders of FX reserves\", ], \n      \"subtitle\": [\"Official foreign exchange reserve (Jan 2022, $tn)\"],\n      \"align\": 'left',\n      \"anchor\": 'start'\n    },\n    width=700,\n    height=512\n)\n\nsource_text = alt.Chart(\n    {\"values\": [{\"text\": \"Source: IMF, \u00a9 FT\"}]}\n).mark_text(size=12, align='left', dx=-140, color='darkgrey').encode(\n    text=\"text:N\"\n)\n\n# from https://stackoverflow.com/questions/57244390/has-anyone-figured-out-a-workaround-to-add-a-subtitle-to-an-altair-generated-cha\nchart = alt.vconcat(\n    square,\n    bars,\n    source_text\n).configure_concat(\n    spacing=0\n).configure(\n    background='#fff1e5',\n).configure_view(\n    stroke=None, # Remove box around graph\n).configure_title(\n    font='metricweb',\n    fontSize=22,\n    fontWeight=400,\n    subtitleFont='metricweb',\n    subtitleFontSize=18,\n    subtitleColor='darkgray',\n    subtitleFontWeight=400,\n    subtitlePadding=15,\n    offset=80,\n    dy=40\n)\n\nchart\n</code></pre> <p>For the moment the font does not look at all to be Metric web :-(</p> <p>A second minor difference are the alignment of the 0.0 and 3.0 labels of the x-axis. In the orginal, those labels are centered. Altair aligns 0.0 to the left and 3.0 to the right.</p> <pre><code>\n</code></pre>"},{"location":"2025/05/25/about-vibe-coding/","title":"About Vibe Coding","text":"<p>Software development is changing rapidly overnight. Indeed:</p> <ol> <li>OpenAI announced Codex  on May 16, 2025, for their $200/month Pro users https://openai.com/index/introducing-codex/.</li> <li>Microsoft GitHub Copilot released its new coding agent on May 19, 2025. https://bsky.app/profile/github.com/post/3lpjxvgje7s2k</li> <li>Google announced a tool called Jules (jules.google.com) on May 20, 2025, making it available for free and</li> <li>Mistral releases devstral, an open-source model for coding agents on May 21, 2025. https://mistral.ai/news/devstral</li> </ol> <p>These new coding agents\u2014along with Cursor, Lovable, Windsurf, V0, Bold.new, and others\u2014are all tools that support some form of \u201cvibe coding\u201d (a term coined by Karpathy indicating AI-assisted coding).</p> <p>This gives rise to a lot of FUD (fear, uncertainty, and doubt) from the corporate gatekeepers. The short-term opportunity is this: in a design thinking approach, a \u201cresearch prototype\u201d that checks the basic hypotheses (who is this product for, what problem does the product solve) can be developed much faster using vibe coding.</p> <p>Even with the expected \u201cvalley of disappointment\u201d that may follow (because users tend to overreact to the initial prototype, which will likely need to be rewritten from scratch), in the end, the chance of building a product that resonates with users is much higher and it will be ready sooner\u2014if the same good old software process is followed, from prototype to Minimum Viable Product (MVP) to Version 1 accepted by users.</p>"},{"location":"2020/12/09/comparing-rt-numbers-for-belgium-on-09-12-2020/","title":"Comparing Rt numbers for Belgium on 09-12-2020","text":"Model Based on URL Rt Date by Niel Hens Cases https://gjbex.github.io/DSI_UHasselt_covid_dashboard/ 0.96 20-12-5 Cori et al. (2013) Hospitalisations https://covid-19.sciensano.be/sites/default/files/Covid19/COVID-19_Weekly_report_NL.pdf 0.798 20-11-27/ till 20-12-3 RKI Hospitalisations https://datastudio.google.com/embed/u/0/reporting/c14a5cfc-cab7-4812-848c-0369173148ab/page/ZwmOB 0.97 20-12-09 rtlive Cases https://rtlive.de/global.html 0.80 20-12-09 epiforecast Cases and Deaths https://epiforecasts.io/covid/posts/national/belgium/ 0.5 20-12-07 Huisman et al. (2020) Cases https://ibz-shiny.ethz.ch/covid-19-re-international/ 1.01 20-11-24 Huisman et al. (2020) Hospitalisations https://ibz-shiny.ethz.ch/covid-19-re-international/ 0.84 20-11-24 RKI Cases https://twitter.com/BartMesuere/status/1336565641764089856 0.99 20-12-08 Deforche (2020) Hospitalisations and Deaths https://twitter.com/houterkabouter/status/1336582281994055680 0.85 20-12-09 SEIR Hospitalisations and Deaths https://twitter.com/vdwnico/status/1336557572254552065 1.5 20-12-09"},{"location":"2025/03/16/creating-a-blogpost-with-a-fastapi-server/","title":"Creating a blogpost with a FastAPI server","text":"<p>I copied the idea from https://github.com/koaning/flask-blogposter/tree/main</p> <p>and vibe coded it into a FlaskAPI app.</p> <p>I reorganized the directory structure and used the uv package manager as explained in this video My 2025 uv-based Python Project Layout for Production Apps made by Hynek Schlawack</p> <p>The repository on github is found here: blog</p>"},{"location":"2025/05/11/the-future-ai-ecosystem-will-be-open/","title":"The future Ai ecosystem will be open","text":"The future AI ecosystem will be open <p>I was just reading the article The walled garden cracks: Nadella bets Microsoft\u2019s Copilots\u2014and Azure\u2019s next act\u2014on A2A/MCP interoperability and this is how I see what's happening in the AI landscape:</p> <ul> <li>Antropic: best user experience and defined MCP (Model Context Protocol)</li> <li>Google: best model on all leaderboards with Gemini and defined A2A (Agent-to-Agent)</li> <li>Microsoft: let's build an open AI ecosystem with MCP and A2A, releases supports for A2A and MCP in VS Code</li> <li>Deepseek: after they pulled of DeepSeek V3, Deepseek released open model DeepSeek-Prover-V2 that tackles advanced theorem proving achieving an 88.9% pass rate on the MiniF2F-test benchmark (Olympiad/AIME level theorems) and solving 49 out of 658 problems on the new PutnamBench. This means that Deepseek is cracking the reasoning part of LLM's.</li> </ul> <p>And OpenAI:</p> <ul> <li>Let us make Gibli images</li> <li>We are not Open anymore because we need to much VC budget. Euhm, actually we want to be open again and let's hire instagram CEO to clean up our mess</li> <li>Here's GPT4.5 preview, euhm, actually GPT 4.1 is better....</li> <li>Buys ~~Cursour~~ Windsurf OpenAI agrees to buy Windsurf for about $3 billion, Bloomberg News reports)</li> </ul> <p>Seeing all those signals, we conclude: the AI future will be open!</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"category/hello/","title":"Hello","text":""},{"location":"category/world/","title":"World","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"archive/2020/page/2/","title":"2020","text":""}]}